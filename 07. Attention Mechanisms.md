# Attention Mechanism  
**RNNs** often fail to maintain long-range dependencies (like connecting distant but related words) and struggle with complex sentence structures. This limitation directly inspired **attention mechanisms**, which allow models to dynamically focus on all relevant parts of the input—solving the bottleneck and memory problems that plagued RNNs.

## RNN Limitations  
**RNNs** were once the go-to solution for language tasks like translation, but they had serious flaws. The main issue was how they handled information. Imagine trying to translate a sentence word by word while only remembering the last few words you saw - that's essentially how RNNs worked. They processed text sequentially, updating their "memory" (hidden state) at each step, but this created problems.

1. First, RNNs struggled with long sentences. Important connections between words far apart (like "cat" at the beginning and "was happy" at the end) often got lost in translation. The network's memory would fade over distance.
2. Second, the entire meaning of a sentence had to squeeze into a single final memory state before translation could even begin. This was like trying to summarize a whole book in one sentence before translating it - you'd inevitably lose crucial details.

These limitations made RNNs bad at handling the complex relationships in language. They motivated researchers to develop attention mechanisms, which let models look back at any part of the original sentence when needed, just like how humans naturally focus on different words when understanding meaning. This breakthrough led to today's much more powerful language models.

## Attention Mechanisms  

Self-attention enables every element in an input sequence to dynamically evaluate and incorporate information from all other elements in that same sequence. This "attention" mechanism determines how much focus each part of the sequence should give to every other part when creating its representation. As the foundational building block of transformer-based models like GPT, self-attention allows modern large language models to understand contextual relationships between all words in a text simultaneously, rather than processing them in isolation or in rigid sequence.  

<img width="685" height="504" alt="image" src="https://github.com/user-attachments/assets/08fee3de-3bf7-4c96-a312-c155646efb98" />

This section dives into implementing and analyzing the self-attention mechanism that powers GPT-style models. This hands-on approach will give you fundamental insights into what makes modern language models work, starting with their most important architectural innovation.

<img width="532" height="451" alt="image" src="https://github.com/user-attachments/assets/20381308-0db2-4356-bfad-84a9c24548ba" />


## Simple Self Attention Mechanism
The term "self" in self-attention highlights how this mechanism evaluates relationships within a single sequence. Unlike traditional attention that connects separate input/output sequences, self-attention examines internal connections analyzing how each word relates to all others in the same sentence. Self attention key properties are as follow:
* Each position (like a word) dynamically determines its relationship to every other position
* Discovers how different elements influence each other (e.g., how verbs connect to their subjects)
* Operates entirely within one input sequence rather than between separate sequences

We'll start with a simplified version (without trainable weights). Figure below demonstrates how self-attention transforms raw token embeddings into context-aware representations:

<img width="668" height="450" alt="image" src="https://github.com/user-attachments/assets/961ab0e2-aadd-44ea-ac89-135553b19e12" />

1. **Input Representation:**
   * A sequence (e.g., "Your journey starts with one step") is converted into token embeddings $x(1)$ to $x(T)$.
   * Each $x(i)$ is a d-dimensional vector (shown as $3D$ in our example).
   * Example: $x(1)$ = "Your", $x(2)$ = "journey" etc.
2. **Context Vector Creation:**
   * For each token (like $x(2)$="journey"), we compute a context vector $z(2)$.
   * $z(2)$ combines $x(2)$ with weighted information from all other tokens.
   * The weights determine how much each token influences $z(2)$.
3. **Why It Matters:**
   * Traditional embeddings represent words in isolation.
   * Context vectors capture how words function together in specific sentences.
   * For example: Bank" in "river bank" vs. "bank account"

**Understanding Attention Scores in Self-Attention:** Attention scores dynamically identify which words matter most to the query, enabling context-aware representations. The first critical step in self-attention is computing attention scores $(w)$, which quantify how strongly each word in a sentence relates to a chosen "**query" word** (here, **$x(2)$ = "journey"**). 

**How It Works:** 
1. **Computing Attention Scores (w)** For the query word ("journey"), we compute its **dot product** (as similarity) with every other word in the sentence (including itself). Words with related meanings (e.g., "journey" and "starts") get higher scores. For example:
   * w_journey, your = dot(x_journey, x_your)
   * w_journey, starts = dot(x_journey, x_starts)
   * ...
   * For vectors $A = [a₁, a₂, a₃]$ and $B = [b₁, b₂, b₃]$: Dot product = $a₁×b₁ + a₂×b₂ + a₃×b₃$
   * Hence, raw attention scores $(w₁, w₂, ..., wₙ)$ is built via dot products.
     
2. **Normalizing Scores to Attention Weights $(a)$** Attention scores are initially unnormalized (e.g., w=0.8 for "journey").These raw scores $(w)$ will be normalized (using softmax) to create attention weights $(a)$ (summing to 1).
   * Turns scores into a probability distribution.
   * Enables the model to "focus" on the most relevant tokens.
   * All weights are positive. (e.g., $a21 + a22 + ... + a2T = 1$).
   * Hence, we have normalized attention weights $(a₁, a₂, ..., aₙ)$, where each weight is between 0 and 1 and all weights sum to 1 (like probabilities)  

     <img width="927" height="414" alt="image" src="https://github.com/user-attachments/assets/c3c85559-7dae-4242-9b8b-62a8c717dfdc" />

3. **Context Vectors** For input embeddings $[x₁, x₂, ..., x₆]$ and attention weights $[a₂₁, a₂₂, ..., a₂₆]$:
   * $z(2)=a21⋅x(1)+a22⋅x(2)+...+a26⋅x(6)$
   * This blends all token embeddings proportionally to their relevance to "journey" to get $z(2)$.
   * z("bank") differs in "river bank" vs "bank account".
   * Replaces static word embeddings with context-aware representations.
   * Example: $z(journey) = 0.6·x(Your) + 0.3·x(journey) + 0.1·x(starts)$.
   * The context vector z(2) is a context-aware version of "journey". A fusion of its own meaning and its relationships to other words in the sentence.

      <img width="990" height="459" alt="image" src="https://github.com/user-attachments/assets/5a3c8cc9-b73d-41f6-b9ca-aaea243f704c" />

We now extend our attention computation from focusing on one token (x(2) = "journey") to processing all tokens:
1. **All-to-All Attention Scores**
   ```
   Input matrix X:        # [6 tokens × 3 dimensions]
   attn_scores = X @ X.T  # [6×6] matrix of raw scores
   ```
   Result:
   
           ```
           [[x₁·x₁, x₁·x₂, ..., x₁·x₆],
           [x₂·x₁, x₂·x₂, ..., x₂·x₆],
             ...
           [x₆·x₁, x₆·x₂, ..., x₆·x₆]]
           ```
3. **Row-wise Softmax Normalization:** Convert scores to probability distributions per token.
4. **Context Vector Synthesis:** Compute weighted sums for all tokens simultaneously.


