# Attention Mechanisms 
**RNNs** often fail to maintain long-range dependencies (like connecting distant but related words) and struggle with complex sentence structures. This limitation directly inspired **attention mechanisms**, which allow models to dynamically focus on all relevant parts of the inputâ€”solving the bottleneck and memory problems that plagued RNNs.

## RNN Limitations  
**RNNs** were once the go-to solution for language tasks like translation, but they had serious flaws. The main issue was how they handled information. Imagine trying to translate a sentence word by word while only remembering the last few words you saw - that's essentially how RNNs worked. They processed text sequentially, updating their "memory" (hidden state) at each step, but this created problems.

1. First, RNNs struggled with long sentences. Important connections between words far apart (like "cat" at the beginning and "was happy" at the end) often got lost in translation. The network's memory would fade over distance.
2. Second, the entire meaning of a sentence had to squeeze into a single final memory state before translation could even begin. This was like trying to summarize a whole book in one sentence before translating it - you'd inevitably lose crucial details.

These limitations made RNNs bad at handling the complex relationships in language. They motivated researchers to develop attention mechanisms, which let models look back at any part of the original sentence when needed, just like how humans naturally focus on different words when understanding meaning. This breakthrough led to today's much more powerful language models.
