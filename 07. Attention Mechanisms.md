# Attention Mechanism  
**RNNs** often fail to maintain long-range dependencies (like connecting distant but related words) and struggle with complex sentence structures. This limitation directly inspired **attention mechanisms**, which allow models to dynamically focus on all relevant parts of the input‚Äîsolving the bottleneck and memory problems that plagued RNNs.

## RNN Limitations  
**RNNs** were once the go-to solution for language tasks like translation, but they had serious flaws. The main issue was how they handled information. Imagine trying to translate a sentence word by word while only remembering the last few words you saw - that's essentially how RNNs worked. They processed text sequentially, updating their "memory" (hidden state) at each step, but this created problems.

1. First, RNNs struggled with long sentences. Important connections between words far apart (like "cat" at the beginning and "was happy" at the end) often got lost in translation. The network's memory would fade over distance.
2. Second, the entire meaning of a sentence had to squeeze into a single final memory state before translation could even begin. This was like trying to summarize a whole book in one sentence before translating it - you'd inevitably lose crucial details.

These limitations made RNNs bad at handling the complex relationships in language. They motivated researchers to develop attention mechanisms, which let models look back at any part of the original sentence when needed, just like how humans naturally focus on different words when understanding meaning. This breakthrough led to today's much more powerful language models.

## Attention Mechanisms  

Self-attention enables every element in an input sequence to dynamically evaluate and incorporate information from all other elements in that same sequence. This "attention" mechanism determines how much focus each part of the sequence should give to every other part when creating its representation. As the foundational building block of transformer-based models like GPT, self-attention allows modern large language models to understand contextual relationships between all words in a text simultaneously, rather than processing them in isolation or in rigid sequence.  

<img width="685" height="504" alt="image" src="https://github.com/user-attachments/assets/08fee3de-3bf7-4c96-a312-c155646efb98" />

This section dives into implementing and analyzing the self-attention mechanism that powers GPT-style models. This hands-on approach will give you fundamental insights into what makes modern language models work, starting with their most important architectural innovation.

<img width="532" height="451" alt="image" src="https://github.com/user-attachments/assets/20381308-0db2-4356-bfad-84a9c24548ba" />


## Simple Self Attention Mechanism
The term "self" in self-attention highlights how this mechanism evaluates relationships within a single sequence. Unlike traditional attention that connects separate input/output sequences, self-attention examines internal connections analyzing how each word relates to all others in the same sentence. Self attention key properties are as follow:
* Each position (like a word) dynamically determines its relationship to every other position
* Discovers how different elements influence each other (e.g., how verbs connect to their subjects)
* Operates entirely within one input sequence rather than between separate sequences

We'll start with a simplified version (without trainable weights). Figure below demonstrates how self-attention transforms raw token embeddings into context-aware representations:

<img width="668" height="450" alt="image" src="https://github.com/user-attachments/assets/961ab0e2-aadd-44ea-ac89-135553b19e12" />

1. **Input Representation:**
   * A sequence (e.g., "Your journey starts with one step") is converted into token embeddings $x(1)$ to $x(T)$.
   * Each $x(i)$ is a d-dimensional vector (shown as $3D$ in our example).
   * Example: $x(1)$ = "Your", $x(2)$ = "journey" etc.
2. **Context Vector Creation:**
   * For each token (like $x(2)$="journey"), we compute a context vector $z(2)$.
   * $z(2)$ combines $x(2)$ with weighted information from all other tokens.
   * The weights determine how much each token influences $z(2)$.
3. **Why It Matters:**
   * Traditional embeddings represent words in isolation.
   * Context vectors capture how words function together in specific sentences.
   * For example: Bank" in "river bank" vs. "bank account"

**Understanding Attention Scores in Self-Attention:** Attention scores dynamically identify which words matter most to the query, enabling context-aware representations. The first critical step in self-attention is computing attention scores $(w)$, which quantify how strongly each word in a sentence relates to a chosen "**query" word** (here, **$x(2)$ = "journey"**). 

**How It Works:** 
1. **Computing Attention Scores (w)** For the query word ("journey"), we compute its **dot product** (as similarity) with every other word in the sentence (including itself). Words with related meanings (e.g., "journey" and "starts") get higher scores. For example:
   * w_journey, your = dot(x_journey, x_your)
   * w_journey, starts = dot(x_journey, x_starts)
   * ...
   * For vectors $A = [a‚ÇÅ, a‚ÇÇ, a‚ÇÉ]$ and $B = [b‚ÇÅ, b‚ÇÇ, b‚ÇÉ]$: Dot product = $a‚ÇÅ√ób‚ÇÅ + a‚ÇÇ√ób‚ÇÇ + a‚ÇÉ√ób‚ÇÉ$
   * Hence, raw attention scores $(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)$ is built via dot products.
     
2. **Normalizing Scores to Attention Weights $(a)$** Attention scores are initially unnormalized (e.g., w=0.8 for "journey").These raw scores $(w)$ will be normalized (using softmax) to create attention weights $(a)$ (summing to 1).
   * Turns scores into a probability distribution.
   * Enables the model to "focus" on the most relevant tokens.
   * All weights are positive. (e.g., $a21 + a22 + ... + a2T = 1$).
   * Hence, we have normalized attention weights $(a‚ÇÅ, a‚ÇÇ, ..., a‚Çô)$, where each weight is between 0 and 1 and all weights sum to 1 (like probabilities)  

     <img width="927" height="414" alt="image" src="https://github.com/user-attachments/assets/c3c85559-7dae-4242-9b8b-62a8c717dfdc" />

3. **Context Vectors** For input embeddings $[x‚ÇÅ, x‚ÇÇ, ..., x‚ÇÜ]$ and attention weights $[a‚ÇÇ‚ÇÅ, a‚ÇÇ‚ÇÇ, ..., a‚ÇÇ‚ÇÜ]$:
   * $z(2)=a21‚ãÖx(1)+a22‚ãÖx(2)+...+a26‚ãÖx(6)$
   * This blends all token embeddings proportionally to their relevance to "journey" to get $z(2)$.
   * z("bank") differs in "river bank" vs "bank account".
   * Replaces static word embeddings with context-aware representations.
   * Example: $z(journey) = 0.6¬∑x(Your) + 0.3¬∑x(journey) + 0.1¬∑x(starts)$.
   * The context vector z(2) is a context-aware version of "journey". A fusion of its own meaning and its relationships to other words in the sentence.
   * Then, we can extend our attention computation from focusing on one token ($x(2)$ = "journey") to processing all tokens.

      <img width="990" height="459" alt="image" src="https://github.com/user-attachments/assets/5a3c8cc9-b73d-41f6-b9ca-aaea243f704c" />

 ## Scaled Dot-Product (trainable weights) Attention  

We now turn to explain the core self-attention mechanism that powers modern transformer architectures like GPT and other state-of-the-art LLMs. This mechanism, formally known as scaled dot-product attention, represents a significant advancement over previous attention approaches by incorporating learnable parameters and optimized scaling. This attention module serves as the fundamental building block within the transformer architecture, enabling the model to dynamically weigh and combine information from all positions in the input sequence. The implementation will introduce three key trainable projection matrices (for **queries, keys, and values**).  

The key innovation here is the addition of learnable weight matrices $(W_q, W_k, W_v)$ that transform the input embeddings into _queries, keys, and values_. Unlike the fixed dot products in basic attention, these trainable parameters allow the model to automatically discover and emphasize the most important relationships between words during training. By adjusting these weights through backpropagation, the attention module learns to generate optimized context vectors that capture meaningful semantic and syntactic patterns, significantly improving the model's language understanding capabilities. This dynamic, learnable approach is what makes modern transformer attention mechanisms so powerful compared to their static predecessors.  

We'll build the self-attention mechanism step-by-step using three learnable weight matrices. First, we transform each input token $x(i)$ into three distinct representations ($@:$ matrix multiplication):
1. $W_q$ creates **query vectors** (what to look for): **Query:** $q(i) = x(i) @ W_q$ 
2. $W_k$ produces key vectors (what to offer): **Key:** $k(i) = x(i) @ W_k$ 
3. $W_v$ generates value vectors (actual content): **Value:** $v(i) = x(i) @ W_v$

Figure bellow, examine how we compute the query, key, and value vectors for our example input $x(2)$ ("journey"):

<img width="911" height="468" alt="image" src="https://github.com/user-attachments/assets/e3df89b4-213e-4b58-b6b0-7597a415c351" />

**Weight Parameters vs. Attention Weights** Here's a clear distinction between these two types of weights:
1. **Weight Parameters $(W_q, W_k, W_v)$:**
   * Fixed matrices learned during training (network parameters)
   * Transform input embeddings into query/key/value spaces
   * Remain constant once trained
     
2. **Attention Weights:**
   * Dynamic values computed for each input sequence
   * Determine how much each token influences others
   * Change based on the specific context

The attention score calculation still relies on dot products to measure relationships between tokens, just like in our earlier simplified version. However, there's a crucial improvement: rather than computing these dot products directly between the raw input embeddings, we first transform them through learned weight matrices. Here's what changes:
1. **Input Projection:** Each input token gets projected into three specialized representations (query vector , key vector , and value vector)
2. **Attention Scores $(œâ)$:** The attention scores are then **calculated as dot products between**:
   * One token's query vector
   * Another token's key vector
3. **attention weights $(ùõº)$:** we convert the raw attention scores $(œâ)$ into proper attention weights $(ùõº)$ using the softmax function. These normalized weights $(ùõº)$ determine how much each token contributes to the final context vector. This transformation does three important things:
   * It scales all scores to a standard range between 0 and 1
   * It ensures all weights for a given token sum to exactly 1
   * It preserves the relative importance of each score while making them comparable

4. **Context vectors:** Each context vector is created by taking a weighted combination of all the value vectors, where the weights come from the attention scores we calculated. 
   
<img width="911" height="627" alt="image" src="https://github.com/user-attachments/assets/e3ff8618-3129-438f-9047-fb76dbd42e1e" />


**Why Query, Key, and Value?**
The query, key, and value system is inspired by how databases retrieve information:
1. **Query (Q) ‚Äì Like a search term**
   * Represents the current word asking"
   * Example: For the word "bank", the query helps decide if it relates to "river" or "money".
2. **Key (K) ‚Äì Like an index in a database**
   * Each word has a key that describes "What information do I have?"
   * The model compares the query to all keys to find the best matches.
3. **Value (V) ‚Äì The actual content being retrieved**
   * Once the best keys are selected, their corresponding values are blended to form the context.
   * Example: If "bank" matches "river", its value contributes more to the final meaning.

## Causal Attention: Preventing Future Peeking in LLMs

When generating text word-by-word (like ChatGPT), the model should only use past and current words to predict the next one. Standard self-attention sees the entire sentence at once, which would let the model "cheat" by using future words. Causal attention fixes this. For example, for the sentence "Your journey starts":
1. Can see: "Your" and "journey"
2. Masked: "starts" (future word)
3. **With masking:** The model learns to predict "starts" using only "Your journey".
4. **Without masking:** It might cheat by using the answer ("starts") during training, then fail when asked to generate text where the answer isn‚Äôt pre-loaded.

Preventing future peeking enables autoregressive generation (Predicts one word at a time left-to-right). This feature is critical for GPT, ChatGPT, and other LLMs. Causal masking is crucial, because you answer each question using only what you‚Äôve learned from previous questions.

<img width="951" height="499" alt="image" src="https://github.com/user-attachments/assets/182b91e0-5941-4320-9286-180c23a8819b" />

**Information Leakage Prevention**  

At first glance, it might seem that future tokens could still influence the model because their scores are included in the softmax calculation. However, masking actually removes them completely from consideration.

When we apply a causal mask, we set future token scores to negative infinity $(-‚àû)$. In the softmax function, this translates to zero influence‚Äîtheir weights become exactly 0, as if they were never part of the calculation. This means:
1. The model only uses unmasked (past and current) tokens to compute attention.
2. Future words are mathematically excluded, ensuring no leakage.
3. The attention weights behave as if the future tokens never existed in the first place.

**Why This Works Perfectly:**
1. The softmax function has a special property: $e^{-‚àû} = 0$. This means any future word gets exactly 0 attention weight.
2. It's not just hidden - it's completely erased from the calculation.
3. The model literally cannot be influenced by future words because their contribution becomes zero.

<img width="752" height="201" alt="image" src="https://github.com/user-attachments/assets/0f7e699b-71a8-485b-8f73-c1e192ec0d5c" />


**Dropout in Attention Mechanisms**

Dropout is a regularization technique that randomly "turns off" (sets to zero) a fraction of neurons or weights during training. This prevents the model from relying too heavily on any single pathway, forcing it to learn more robust and generalizable patterns.  In transformer models like GPT, dropout can be strategically inserted at two key points in the attention mechanism:
1. **After computing attention weights:** Applied to attention weights post-softmax (most common)
2. **After applying weights to values:** Applied to the final weighted value vectors.

**Why we use dropout in attention?**
1. Avoids Overconfidence: Prevents the model from relying on a fixed set of attention patterns.
2. Smoother Training: Reduces the risk of the model "overfitting" to specific sequences.
3. Better Long-Text Handling: Ensures attention remains flexible across different contexts.

**The Dropout Process**  
For a dropout rate of 50%:
1. Randomly zero out 50% of attention weights (per training step).
2. Scale up the remaining weights by 2√ó (to preserve the total "attention budget").

**Example**  

Original weights: $[0.6, 0.3, 0.1]$, and after $50%$ dropout (if $0.3$ is dropped):
* Masked: $[0.6, 0.0, 0.1]$.
* Scaled: $[1.2, 0.0, 0.2]$ (weights sum to $~1.4$ instead of $1.0$, but gradients adjust accordingly)

<img width="977" height="1057" alt="image" src="https://github.com/user-attachments/assets/f3a39e24-2ccb-48a2-ac3d-289ce8364660" />

## Multi-Head Attention

We now reach the final stage of our attention mechanism implementation: transforming our single-head causal attention into multi-head attention. This powerful extension divides the attention process into multiple parallel "heads" that each learn to focus on different aspects of the input sequence independently. While our current single-head version processes information sequentially with one set of attention weights, the multi-head approach enables simultaneous analysis through multiple specialized attention channels. Think of multi-head attention like having multiple detectives working on a case together:
1. Instead of one detective (single-head attention), we have several (e.g., 8 detectives).
2. Each detective examines the same evidence (words) independently.
3. Each has their own notebook (separate weights) to record findings.
4. All detectives look at the case simultaneously (in parallel).
5. While one might focus on verb relationships, another might track pronoun references.
6. After all detectives finish, their reports are combined.
7. This gives a more complete understanding than any single detective could provide.

<img width="1004" height="854" alt="image" src="https://github.com/user-attachments/assets/8bed980d-6a4e-4859-830b-cf235a9fcb0b" />

While this approach uses more computing power, it's essential for understanding complex language patterns that a single perspective might miss. This is why all advanced language models use this multi-head approach.

<img width="693" height="413" alt="image" src="https://github.com/user-attachments/assets/cc9efc6c-176e-4326-9eac-bc0c1d9733fe" />



