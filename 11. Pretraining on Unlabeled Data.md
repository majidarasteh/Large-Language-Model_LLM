# Pretraining on Unlabeled Data
Pretraining is the foundational stage where a Large Language Model (LLM) learns the statistical patterns, grammar, facts, and reasoning abilities of a language by predicting the next word in a vast corpus of text. This process is self-supervised; the data is unlabeled because the "label" for any given word is simply the next word in the sequence.

## Test generation using GPT

Generating text with a model like GPT is a three-step cycle:  
1. **Tokenization:** The input text is converted into a sequence of token IDs using a tokenizer (e.g., GPT-2's BPE tokenizer).
2. **Model Inference:** The sequence of token IDs is fed into the LLM. The model's final output is a tensor of logits for each position in the sequence. These logits represent scores for every token in the model's vocabulary, indicating the model's prediction for the next token.
3. **Decoding:** The logits are converted back into token IDs. This can be done deterministically (e.g., always choosing the token with the highest score, "greedy decoding") or stochastically (e.g., using temperature or top-k sampling to introduce creativity). Finally, the new token IDs are converted back into human-readable text by the tokenizer.

This process is repeated autoregressively: the newly generated token is appended to the input, and the cycle continues until a stopping condition is met (e.g., a maximum length or an end-of-sequence token is generated). 

<img width="869" height="483" alt="image" src="https://github.com/user-attachments/assets/5cf810ab-8692-4108-a2cc-24cc8c270a1c" />

##  Evaluating Generative Text Models
Once text is generated, we need ways to measure its quality. Evaluating the quality of text generated by a Large Language Model (LLM) is a critical step in the development process. Basic evaluation methods include:  

1. **Loss-based Metrics (Intrinsic Evaluation):**
   * Cross-Entropy Loss / Perplexity: The most fundamental quantitative measure. It calculates how well the model's predicted probability distribution matches the actual distribution of words in a held-out test dataset.
   * Lower Perplexity indicates the model is more "confident" and accurate in its predictions. This is an automatic, inexpensive metric that is core to the training process but doesn't always perfectly correlate with human judgment of quality.
2. **Human Evaluation (Extrinsic Evaluation):** The gold standard for evaluation. Humans rate generated text based on criteria like:
   * Coherence: Does the text make sense and flow logically?
   * Fluency: Is the text grammatically correct and natural-sounding?
   * Relevance: Does the text stay on-topic and address the prompt?
   * Factual Accuracy: For non-fiction, is the information correct?
     
Evaluation involves a combination of automatic metrics (like loss) to track progress during training and human-centric methods (inspection and scoring) to ultimately assess the usefulness and quality of the generated text.

## Calculating the Text Generation Loss
The text generation loss is a numerical score that quantifies how well the model's predictions match the actual text data. The loss function provides a single, automated metric to assess the quality of the model's output during training. It is the foundation for training the model, as the goal is to minimize this loss.

Figure below outlines the text generation process using a simplified seven-token vocabulary for illustrative clarity, the actual GPTModel operates on a vastly larger scale. In practice, the model utilizes a extensive vocabulary consisting of 50,257 unique tokens. Furthermore, this figure utilizes a single text example ('every effort moves') to demonstrate the generation process in a simplified and clear manner.

<img width="860" height="527" alt="image" src="https://github.com/user-attachments/assets/537e8836-ff89-4ad0-bcb6-f0747e8019cf" />

To calculating text generation loss, first, we need example data. The input is a sequence of tokens. The target is the same sequence, but shifted one token to the left. This teaches the model to predict the next word. For example:  

* Input Text: "Every effort moves"
* Input Text: "I really like"
* Target Text: "effort moves you"
* Target Text: "really like chocolate:

These are converted to token IDs using the tokenizer:
* Input IDs: torch.tensor([16833, 3626, 6100]) --->  (for ['Every', 'effort', 'moves'])
* Input IDs: torch.tensor([40, 1107, 588])     --->  (for ['I', 'really', 'like'])
* Target IDs: torch.tensor([3626, 6100, 345])  --->  (for ['effort', 'moves', 'you'])
* Target IDs: torch.tensor([1107, 588, 11311]) ---> (for ['really', 'like', 'chocolate'])

The input IDs are fed into the model. The model doesn't output text directly; it outputs logits for every token in the vocabulary at every position.
* Output Shape: [batch_size, num_tokens, vocab_size]
* Foe Example: The resulting tensor containing the probability scores has the dimensions torch.Size([2, 3, 50257]), which encapsulates the model's predictions for the entire batch. The first dimension, 2, represents the batch size, corresponding to the two input examples processed simultaneously. The second dimension, 3, indicates the number of tokens in each input sequence. Finally, the third dimension, 50,257, defines the size of the model's vocabulary, representing the probability distribution over every possible next token for each position. This tensor is generated following the conversion of the model's raw output logits into a normalized probability distribution via the softmax function, which ensures all values sum to one and are interpretable as probabilities.
