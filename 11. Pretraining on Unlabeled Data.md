# Pretraining on Unlabeled Data
Pretraining is the foundational stage where a Large Language Model (LLM) learns the statistical patterns, grammar, facts, and reasoning abilities of a language by predicting the next word in a vast corpus of text. This process is self-supervised; the data is unlabeled because the "label" for any given word is simply the next word in the sequence.

## Test generation using GPT

Generating text with a model like GPT is a three-step cycle:  
1. **Tokenization:** The input text is converted into a sequence of token IDs using a tokenizer (e.g., GPT-2's BPE tokenizer).
2. **Model Inference:** The sequence of token IDs is fed into the LLM. The model's final output is a tensor of logits for each position in the sequence. These logits represent scores for every token in the model's vocabulary, indicating the model's prediction for the next token.
3. **Decoding:** The logits are converted back into token IDs. This can be done deterministically (e.g., always choosing the token with the highest score, "greedy decoding") or stochastically (e.g., using temperature or top-k sampling to introduce creativity). Finally, the new token IDs are converted back into human-readable text by the tokenizer.

This process is repeated autoregressively: the newly generated token is appended to the input, and the cycle continues until a stopping condition is met (e.g., a maximum length or an end-of-sequence token is generated). 

<img width="869" height="483" alt="image" src="https://github.com/user-attachments/assets/5cf810ab-8692-4108-a2cc-24cc8c270a1c" />

##  Evaluating Generative Text Models
Once text is generated, we need ways to measure its quality. Evaluating the quality of text generated by a Large Language Model (LLM) is a critical step in the development process. Basic evaluation methods include:  

1. **Loss-based Metrics (Intrinsic Evaluation):**
   * Cross-Entropy Loss / Perplexity: The most fundamental quantitative measure. It calculates how well the model's predicted probability distribution matches the actual distribution of words in a held-out test dataset.
   * Lower Perplexity indicates the model is more "confident" and accurate in its predictions. This is an automatic, inexpensive metric that is core to the training process but doesn't always perfectly correlate with human judgment of quality.
2. **Human Evaluation (Extrinsic Evaluation):** The gold standard for evaluation. Humans rate generated text based on criteria like:
   * Coherence: Does the text make sense and flow logically?
   * Fluency: Is the text grammatically correct and natural-sounding?
   * Relevance: Does the text stay on-topic and address the prompt?
   * Factual Accuracy: For non-fiction, is the information correct?
     
Evaluation involves a combination of automatic metrics (like loss) to track progress during training and human-centric methods (inspection and scoring) to ultimately assess the usefulness and quality of the generated text.

## Calculating the Text Generation Loss
The text generation loss is a numerical score that quantifies how well the model's predictions match the actual text data. The loss function provides a single, automated metric to assess the quality of the model's output during training. It is the foundation for training the model, as the goal is to minimize this loss.

Figure below outlines the text generation process using a simplified seven-token vocabulary for illustrative clarity, the actual GPTModel operates on a vastly larger scale. In practice, the model utilizes a extensive vocabulary consisting of 50,257 unique tokens. Furthermore, this figure utilizes a single text example ('every effort moves') to demonstrate the generation process in a simplified and clear manner.

<img width="860" height="527" alt="image" src="https://github.com/user-attachments/assets/537e8836-ff89-4ad0-bcb6-f0747e8019cf" />

To calculating text generation loss, first, we need example data. The input is a sequence of tokens. The target is the same sequence, but shifted one token to the left. This teaches the model to predict the next word. For example:  

**Step 1: Prepare the Batch** You have two input sequences and their corresponding target sequences (the correct next tokens).

* Input Text: "Every effort moves"
* Input Text: "I really like"
* Target Text: "effort moves you"
* Target Text: "really like chocolate:

These are converted to token IDs using the tokenizer:
* Input IDs: ```torch.tensor([16833, 3626, 6100])``` --->  (for ['Every', 'effort', 'moves'])
* Input IDs: ```torch.tensor([40, 1107, 588])```     --->  (for ['I', 'really', 'like'])
* Target IDs: ```torch.tensor([3626, 6100, 345])```  --->  (for ['effort', 'moves', 'you'])
* Target IDs: ```torch.tensor([1107, 588, 11311])``` ---> (for ['really', 'like', 'chocolate'])

These are combined into single tensors for the batch:
* ```inputs = torch.tensor([[16833, 3626, 6100], [40, 1107, 588]])```
* ```targets = torch.tensor([[3626, 6100, 345], [1107, 588, 11311]])```

You have two input sequences and their corresponding target sequences (the correct next tokens).

**Step 2: Get the Model's Predictions (Logits)** The input tensor is fed into the model. The model doesn't output text; it outputs logits (scores) for every token in its vocabulary for every position in the input.

* **Output Shape:** ```[batch_size, num_tokens, vocab_size]```
* For our batch: The output is a tensor of shape [2, 3, 50257].
  - 2 examples in the batch.
  - 3 tokens per example.
  - 50,257 scores (logits) for each token position.

**Step 3: Calculate the Loss (Cross-Entropy)** We now compare the model's predictions (logits) to the true answers (targets).
1. **Flatten the Tensors:** We reshape the tensors to treat every prediction independently.
   * Logits: Shape [2, 3, 50257] is reshaped to [6, 50257]. We now have 6 total predictions to check (3 tokens per batch × 2 batchs).
   * Targets: Shape [2, 3] is reshaped to [6]. We now have 6 correct answers.
     
2. **Apply Cross-Entropy Loss:**
   * The F.cross_entropy function does the calculation. For each of the 6 positions, it:
     - Applies softmax to the 50,257 logits to get a probability distribution.
     - Checks the probability the model assigned to the correct token (the target ID).
     - Takes the negative logarithm of that probability.
    * Example: For the first target ID $3626$ (which is "effort"), if the model's probability is low (e.g., $0.1$), the loss for that prediction is high: $-log(0.1) ≈ 2.3$. If the probability is high (e.g., $0.9$), the loss is low: $-log(0.9) ≈ 0.1$.
      
3. **Get the Final Score:**
   * The losses for all 6 individual predictions are averaged into a single number. This is the final loss value.

## Backpropagation and the Cross-Entropy Loss
The mechanism for maximizing the correct token probabilities is backpropagation. This process calculates how each model weight contributes to the final error, providing a precise roadmap for optimization.

1. **The Loss Function as a Guide:** Backpropagation requires a loss function to quantify the model's error. In this case, the error is the difference between the model's predicted probability distribution and the ideal distribution where the correct token has a probability of 1.
2. **The Manual Calculation**
   * For a given input and target, extract the model's predicted probability for the correct next token at each position (_target_probas_1_ and _target_probas_2_).
     - e.g. ```target_probas_1 = [7.4541e-05, 3.1061e-05, 1.1563e-05]```
     - e.g. ```target_probas_2 = [1.0337e-05, 5.6776e-05, 4.7559e-06]```
   *  Apply the logarithm to these probabilities. Since probabilities are $≤1$, their logarithms are $≤0$. A higher probability (e.g., $0.9$) results in a less negative log value (~-0.1) than a low probability (e.g., 0.1, which has a log of ~-2.3).
     - Resulting Log Probabilities: ```tensor([ -9.5042, -10.3796, -11.3677, -11.4798, -9.7764, -12.2561])```
     - The first three values are the log-probs for Text 1.
     - The last three values are the log-probs for Text 2.
   *  Average these log probabilities. The goal is to maximize this average, bringing it as close to 0 as possible (which corresponds to predicting the correct token with $100%$ confidence).
   * - Calculate the average log probability:
     - ```avg_log_probas = mean([ -9.5042 + -10.3796 + -11.3677 + -11.4798 + -9.7764 + -12.2561 ])```
     - ```avg_log_probas = mean([-64.7638]) ≈ -10.794```
   *  In practice, we minimize a loss function. Therefore, we take the negative average log probability, known as the **cross-entropy loss**. This transforms our goal from "maximize the average log prob" to "minimize the negative average log prob," pushing the loss value toward 0.
   * - ```neg_avg_log_probas = avg_log_probas  × -1```
     - ```neg_avg_log_probas = -10.794 × -1 = 10.794```
     - Your final loss value is 10.794.
3. **From Calculation to Optimization:** The calculated loss value is a single number representing the model's total error. Backpropagation uses calculus (the chain rule) to compute the gradient of this loss with respect to every model parameter. The optimizer then uses these gradients to update the weights, adjusting them in a way that is calculated to reduce the loss on the next iteration. This cycle of prediction, loss calculation, and weight adjustment via backpropagation is the fundamental process that trains the neural network.

### Cross-Entropy Loss for LLMs
The cross-entropy loss is the fundamental mechanism for training LLMs. It quantifies the difference between the model's predicted probability distribution and the true distribution (where the correct next token has a probability of 1).
1. **Interchangeable Terms:** In practice, **"cross-entropy loss"** and **"negative average log probability"** are synonymous for this task. Both describe the same calculation: measuring how surprised the model is by the correct answer.
   
2. **Tensor Shapes:** The operation requires aligning two tensors:
   - Logits: The model's raw predictions with shape ```[batch_size, num_tokens, vocab_size]```
   - Targets: The correct token IDs with shape ```[batch_size, num_tokens]```
     
3. **Flattening:** To compute the loss across the entire batch, these tensors are flattened. This combines all batch elements and sequence positions into one long list of individual predictions to evaluate.
   - ```logits_flat: [batch_size * num_tokens, vocab_size]``` (e.g., [6, 50257])
   - ```targets_flat: [batch_size * num_tokens]``` (e.g., [6])
     
4. **PyTorch Simplification:** Instead of manually calculating probabilities, taking logs, and averaging, PyTorch's ```F.cross_entropy(logits_flat, targets_flat)``` function performs all these steps efficiently in one go. It internally applies softmax to the logits and then computes the negative log likelihood for the target indices.
   
5. **The Result:** The output is a single, scalar loss value (e.g., 10.794). This high value indicates a poorly performing, untrained model. The goal of training is to adjust the model's weights to minimize this value, pushing it towards zero.

### Perplexity
Perplexity is an intuitive metric derived from the cross-entropy loss, used specifically to evaluate language models. It transforms the abstract loss value into a more meaningful concept: the effective number of choices the model feels it has when predicting the next token.

1. **Calculation:** Perplexity is defined as the exponential of the loss.
   * ```Perplexity = exp(Loss)```
   * In the example: ```exp(10.794) ≈ 48,726```
     
2. **Interpretation:** A lower perplexity is better. It indicates the model is more confident and accurate in its predictions.
   * **Perfect Prediction:** A loss of 0 would give a perplexity of 1 (```exp(0) = 1```). This means the model is perfectly confident and has no uncertainty about the next token (it always assigns a probability of 1.0 to the correct choice).
   * **Random Guessing:** A model that predicts uniformly at random from a vocabulary of 50,257 tokens would have a loss of ```log(50257) ≈ 10.82``` and a perplexity of 50,257. This is the worst-case baseline.

3. **Intuitive Meaning:** Your model's perplexity of ~48,726 is extremely high and very close to the random guessing baseline of 50,257. This means the model is essentially no better than random guessing. At every step, it perceives the next token as being chosen from a set of roughly 48,725 equally likely possibilities.

## The Cost of Pretraining LLMs
Pretraining large language models is an exceptionally resource-intensive endeavor, far beyond the capabilities of most individuals or organizations. Imagine you want to teach a computer to understand and write perfect English by having it read almost every book on the internet. The process of doing this is called pretraining, and it's incredibly expensive. Here’s why:
1. **The Model is Massive:** A modern LLM isn't a simple program; it's a gigantic network of billions of "parameters". For example, a **well-known model called Llama 2 has 7 billion** of these parameters. Adjusting all of them perfectly requires a mind-boggling amount of calculation.
2. **It Needs to Read Everything:** To become smart, the model must read an almost unimaginable amount of text. The Llama 2 model was trained on **2 trillion pieces of words (tokens)**. That's like reading a massive library of books millions of times over.
3. **It Requires Supercomputers, Not Just Laptops:** You can't run this on a home computer. It requires thousands of the most powerful specialized processors **(called GPUs, like NVIDIA A100s)** running non-stop for months.
   * **Time:** The Llama 2 model needed **184,320 hours** of time on these **A100 GPUs**.
   * **Cost:** Renting that kind of firepower from a **cloud provider like AWS** costs a fortune. A rough estimate for this single training run is **$690,000**.

This is why, we don't pretrain a model from scratch. Instead of attempting to pretrain a model from scratch which is a prohibitively expensive process, we focus on:
1. Working with a **small, manageable dataset** for educational purposes.
2. **Loading pretrained weights** from openly available models **like GPT-2**. This allows us to skip the incredibly costly pretraining phase and focus on understanding the architecture and learning the crucial. Therefore, we download a **model like GPT-2** that has already been pretrained by someone else (like **OpenAI**) who has already paid that huge cost. 
