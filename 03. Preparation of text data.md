# Data Preparation

Deep neural networks, including large language models (LLMs), cannot directly process raw text because it is categorical and incompatible with mathematical operations like matrix multiplication. To address this, text must be converted into continuous-valued vector representations through a process called embedding. Embeddings transform discrete tokens (words or subwords) into dense numerical vectors that capture semantic relationships, enabling neural networks to process and learn from text. Different data types (e.g., text, audio, video) require specialized embedding models. Techniques like tokenization (e.g., byte pair encoding) and positional encoding further refine these embeddings to preserve word order and context. This preprocessing step is essential for training models to understand and generate human-like text.  

<img width="662" height="402" alt="image" src="https://github.com/user-attachments/assets/f7c90f63-d33c-4264-a941-52d1cfb7b6a6" />

Several algorithms have been developed to create word embeddings, with **Word2Vec** being one of the earliest and most influential. Word2Vec trains a neural network to generate embeddings. The key insight is that words appearing in similar contexts share related meanings, allowing their embeddings to capture semantic relationships. While pretrained models like Word2Vec provide general-purpose embeddings, modern large language models (LLMs) typically generate their own embeddings as part of the input layer, which are fine-tuned during training. This approach offers a key advantage: embeddings are optimized specifically for the model's task and dataset, leading to better performance compared to static, pretrained embeddings like Word2Vec.  

Word embeddings can vary in dimensionality, ranging from just a few dimensions to thousands. Higher-dimensional embeddings (e.g., hundreds or thousands of features) can capture more nuanced semantic relationships between words, enabling models to discern subtle contextual differences. However, this increased expressiveness comes at a computational cost—larger embeddings require more memory and processing power, potentially slowing down training and inference. Lower-dimensional embeddings, while less precise, offer greater efficiency. The choice of dimensionality involves a trade-off: models like GPT-3 use high-dimensional embeddings (e.g., 12,288 dimensions) for advanced performance, whereas simpler applications may prioritize computational efficiency with smaller embeddings. Balancing expressiveness and efficiency is key in designing effective NLP systems.

##  Text Tokenization
Tokenization is the fundamental process of breaking down input text into smaller units called tokens, which serve as the basic building blocks for LLMs. These tokens can represent individual words, subwords, or special characters (including punctuation). Tokenization transforms raw text into a structured sequence that can be converted into numerical embeddings for model processing. For example, the sentence "Hello, world!" might be tokenized into ["Hello", ",", "world", "!"]. Effective tokenization is crucial. It balances granularity (preserving meaning) and efficiency (managing vocabulary size), with methods ranging from simple whitespace splitting to advanced algorithms like byte pair encoding (BPE) used in models like GPT. This preprocessing step directly impacts an LLM's ability to interpret and generate human-like text.  

<img width="563" height="628" alt="image" src="https://github.com/user-attachments/assets/1f3cc79a-45f9-487e-a307-866ef6dce8c0" />

## Mapping tokens into token IDs
After tokenizing text into individual units (words, subwords, or symbols), the next step is converting these tokens into numerical token IDs which is a prerequisite for generating embedding vectors. This process requires building a vocabulary that maps each unique token to a distinct integer identifier. For instance, in a simple vocabulary, "cat" might map to 42, while "," maps to 3. This numerical representation enables mathematical operations in neural networks. The vocabulary is typically constructed by assigning IDs to tokens based on frequency or alphabetical order, ensuring consistent encoding across the dataset. Token IDs serve as indices to retrieve corresponding embedding vectors during model processing, bridging raw text to computable data for LLMs.  

<img width="832" height="624" alt="image" src="https://github.com/user-attachments/assets/35d27090-b33e-4c55-a518-a1c66362a4a6" />

The vocabulary dictionary maps each token to a unique integer identifier. Next, we'll use this mapping to transform raw text into numerical token IDs.  

<img width="855" height="605" alt="image" src="https://github.com/user-attachments/assets/f23849ae-d1e6-44f7-922e-2e59fbca9c80" />

## Special Tokens  

When building your own text processing system, you need to plan for special cases in the vocabulary (your word dictionary). Two important special tokens to include are:
1. **<|unk|>** - This acts as a "placeholder" for words your system hasn't seen before. Like when you encounter a new word not in your dictionary, you use this instead.
2. **<|endoftext|>** - This works like a "chapter break" to separate different documents or conversations. It tells the system when one piece of text ends and another begins.

<img width="903" height="579" alt="image" src="https://github.com/user-attachments/assets/32bc52d0-b668-45f4-87a1-2e643308d1f4" />

## Byte Pair Encoding (BPE)
The **Byte Pair Encoding (BPE)** method gives models like GPT a clever way to process all words, even unfamiliar ones. Here's how it works:
1. **Special Token Notes**  
   * The **<|endoftext|>** marker (ID 50256) acts like a "stop sign" between documents.
   * Unlike simpler systems, BPE doesn't need an **<|unk|>** placeholder.
2. **The Magic of Subwords**  
   * When BPE sees an unknown word (like "someunknownPlace"), it doesn't give up
   * Instead, it breaks the word into smaller pieces it does know:
     * Example: "some" + "unknown" + "Place"
     * Or even individual letters if needed. for instance, if no subwords match, uses individual letters:
       * Example: "Zqx" → "Z" + "q" + "x"
3. **Vocabulary System**
   * Has 50,257 word "slots" in its dictionary
   * Common words get their own slot (e.g., "the")
   * Rare words are built from smaller pieces

BPE gives AI models much better language flexibility compared to systems that just mark unknown words as <|unk|>. It's why GPT models can understand specialized terms, creative spellings, and evolving vocabulary so effectively.
     
<img width="687" height="294" alt="image" src="https://github.com/user-attachments/assets/1f8e6890-0d48-415f-825d-681e2af258a0" />








