# Data Preparation

Deep neural networks, including large language models (LLMs), cannot directly process raw text because it is categorical and incompatible with mathematical operations like matrix multiplication. To address this, text must be converted into continuous-valued vector representations through a process called embedding. Embeddings transform discrete tokens (words or subwords) into dense numerical vectors that capture semantic relationships, enabling neural networks to process and learn from text. Different data types (e.g., text, audio, video) require specialized embedding models. Techniques like tokenization (e.g., byte pair encoding) and positional encoding further refine these embeddings to preserve word order and context. This preprocessing step is essential for training models to understand and generate human-like text.  

<img width="662" height="402" alt="image" src="https://github.com/user-attachments/assets/f7c90f63-d33c-4264-a941-52d1cfb7b6a6" />

Several algorithms have been developed to create word embeddings, with **Word2Vec** being one of the earliest and most influential. Word2Vec trains a neural network to generate embeddings. The key insight is that words appearing in similar contexts share related meanings, allowing their embeddings to capture semantic relationships. While pretrained models like Word2Vec provide general-purpose embeddings, modern large language models (LLMs) typically generate their own embeddings as part of the input layer, which are fine-tuned during training. This approach offers a key advantage: embeddings are optimized specifically for the model's task and dataset, leading to better performance compared to static, pretrained embeddings like Word2Vec.  

Word embeddings can vary in dimensionality, ranging from just a few dimensions to thousands. Higher-dimensional embeddings (e.g., hundreds or thousands of features) can capture more nuanced semantic relationships between words, enabling models to discern subtle contextual differences. However, this increased expressiveness comes at a computational cost—larger embeddings require more memory and processing power, potentially slowing down training and inference. Lower-dimensional embeddings, while less precise, offer greater efficiency. The choice of dimensionality involves a trade-off: models like GPT-3 use high-dimensional embeddings (e.g., 12,288 dimensions) for advanced performance, whereas simpler applications may prioritize computational efficiency with smaller embeddings. Balancing expressiveness and efficiency is key in designing effective NLP systems.

##  Text Tokenization
Tokenization is the fundamental process of breaking down input text into smaller units called tokens, which serve as the basic building blocks for LLMs. These tokens can represent individual words, subwords, or special characters (including punctuation). Tokenization transforms raw text into a structured sequence that can be converted into numerical embeddings for model processing. For example, the sentence "Hello, world!" might be tokenized into ["Hello", ",", "world", "!"]. Effective tokenization is crucial. It balances granularity (preserving meaning) and efficiency (managing vocabulary size), with methods ranging from simple whitespace splitting to advanced algorithms like byte pair encoding (BPE) used in models like GPT. This preprocessing step directly impacts an LLM's ability to interpret and generate human-like text.  

<img width="563" height="628" alt="image" src="https://github.com/user-attachments/assets/1f3cc79a-45f9-487e-a307-866ef6dce8c0" />

## Mapping tokens into token IDs
After tokenizing text into individual units (words, subwords, or symbols), the next step is converting these tokens into numerical token IDs which is a prerequisite for generating embedding vectors. This process requires building a vocabulary that maps each unique token to a distinct integer identifier. For instance, in a simple vocabulary, "cat" might map to 42, while "," maps to 3. This numerical representation enables mathematical operations in neural networks. The vocabulary is typically constructed by assigning IDs to tokens based on frequency or alphabetical order, ensuring consistent encoding across the dataset. Token IDs serve as indices to retrieve corresponding embedding vectors during model processing, bridging raw text to computable data for LLMs.  

<img width="832" height="624" alt="image" src="https://github.com/user-attachments/assets/35d27090-b33e-4c55-a518-a1c66362a4a6" />

The vocabulary dictionary maps each token to a unique integer identifier. Next, we'll use this mapping to transform raw text into numerical token IDs.  

<img width="855" height="605" alt="image" src="https://github.com/user-attachments/assets/f23849ae-d1e6-44f7-922e-2e59fbca9c80" />

## Special Tokens  

When building your own text processing system, you need to plan for special cases in the vocabulary (your word dictionary). Two important special tokens to include are:
1. **<|unk|>** - This acts as a "placeholder" for words your system hasn't seen before. Like when you encounter a new word not in your dictionary, you use this instead.
2. **<|endoftext|>** - This works like a "chapter break" to separate different documents or conversations. It tells the system when one piece of text ends and another begins.

<img width="903" height="579" alt="image" src="https://github.com/user-attachments/assets/32bc52d0-b668-45f4-87a1-2e643308d1f4" />

## Byte Pair Encoding (BPE)
The **Byte Pair Encoding (BPE)** method gives models like GPT a clever way to process all words, even unfamiliar ones. Here's how it works:
1. **Special Token Notes**  
   * The **<|endoftext|>** marker (ID 50256) acts like a "stop sign" between documents.
   * Unlike simpler systems, BPE doesn't need an **<|unk|>** placeholder.
2. **The Magic of Subwords**  
   * When BPE sees an unknown word (like "someunknownPlace"), it doesn't give up
   * Instead, it breaks the word into smaller pieces it does know:
     * Example: "some" + "unknown" + "Place"
     * Or even individual letters if needed. for instance, if no subwords match, uses individual letters:
       * Example: "Zqx" → "Z" + "q" + "x"
3. **Vocabulary System**
   * Has 50,257 word "slots" in its dictionary
   * Common words get their own slot (e.g., "the")
   * Rare words are built from smaller pieces

BPE gives AI models much better language flexibility compared to systems that just mark unknown words as <|unk|>. It's why GPT models can understand specialized terms, creative spellings, and evolving vocabulary so effectively.
     
<img width="687" height="294" alt="image" src="https://github.com/user-attachments/assets/1f8e6890-0d48-415f-825d-681e2af258a0" />


## Data Sampling with Sliding Window for LLM Training

Sliding windows split text into overlapping chunks. Each input is a sequence, and the target is the next word. Sliding window helps LLMs learn language patterns efficiently. This method is fundamental in training models like ChatGPT. The sliding window is a fundamental tool for breaking long sequences into manageable, context-rich chunks. When training a Large Language Model (LLM), we need input–target pairs—sequences of text where:

1. **Input:** A partial sentence (context window).
2. **Target:** The next word (or token) the model should predict.

This is done using a sliding window approach. Step-by-Step explanation of sliding window is as follow:  

1. **Sliding Over Text**
   * The text is split into chunks (tokens/words).
   * A fixed-size window moves over the text, creating overlapping sequences.
2. **Creating Input–Target Pairs:** For each window position:
   * Input: A sequence of $n$ tokens.
   * Target: The next token after the input.

**Example:**  

<img width="595" height="364" alt="image" src="https://github.com/user-attachments/assets/851849cb-ccba-4b01-b541-98e6c6402e83" />


**Why Use a Sliding Window?**  
1. **Efficient Training:**
   * Reuses text data in overlapping windows.
   * Maximizes learning from limited data.
2. **Teaches Context Prediction:**
   * The model learns to predict the next word based on previous words.
   * Helps in autoregressive generation (like ChatGPT).
3. **Flexible Window Size:**
   * Can adjust n (e.g., n=64, n=128) based on model needs.

***Implementation***  

Before we can convert tokens into numerical embeddings for a language model, we first need to structure our data for efficient processing. This is where a Data Loader comes in. The Data Loader's job is to split the text into meaningful chunks of input-target pairs, where the input is a sequence of tokens and the target is the next token the model should predict. A tokenizer automatically handles the conversion from words to numerical token IDs, allowing the Data Loader to work with these numbers directly rather than raw text. By organizing the data into batches, the Data Loader significantly speeds up training, as the model can process multiple input-target pairs simultaneously rather than one at a time. Importantly, both the inputs and targets are represented as token IDs, which are numerical representations of the text that the model can understand and process.

To prepare the data for the Data Loader, we first create a custom class that inherits from PyTorch’s Dataset class. This class is responsible for taking the raw text, splitting it into input-target pairs (e.g., an input sequence and its corresponding next token), and providing these pairs one at a time when requested. The Dataset class has two key methods: ```__len__```, which tells us how many data pairs exist in the dataset, and ```__getitem__```, which retrieves a specific pair by its index. For example, if our dataset contains 1,000 input-target pairs, ```__len__``` returns 1,000, while ```__getitem__(5)``` fetches the 6th pair (since indexing starts at 0). This structured approach ensures the Data Loader can efficiently access and batch the data, streamlining the training process for the language model. The following picture shows the input-target pairs in LLM.

<img width="765" height="405" alt="image" src="https://github.com/user-attachments/assets/7109ebb3-6e65-44be-be99-537ac0e42347" />

**Some important concepts in sliding window**  

1. **Context-Size (window-size) in LLMs:** Context size (also called context window) is the maximum number of tokens (words/subwords) a language model can consider at once when generating text.
2. **Input-Target pairs:** Input-target pairs are the building blocks for training language models like GPT. They teach the model to predict the next word (or token) in a sequence.
   * Input: A chunk of text (context)
   * Target: The next word/token that should follow
3. **Stride:** Stride determines how much the sliding window moves forward when creating input-target pairs from text. It controls the overlap between consecutive sequences during training.
4. **Sliding-window:** A sliding window is a technique that processes sequential data (like text) by moving a fixed-size "window" across the input to extract smaller, overlapping chunks for analysis or training. Its key characteristics:
   * Fixed Window Size (max_length): Determines how many tokens/words are visible at once (e.g., 4 words).
   * Stride (step): Controls how far the window moves forward each step (e.g., 1 word = max overlap and if stride = window size no overlap).
   * Balances Context & Efficiency:
     * Small windows → Faster training but limited context.
     * Large windows → Better context but higher memory use.




