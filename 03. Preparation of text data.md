# Data Preparation

Deep neural networks, including large language models (LLMs), cannot directly process raw text because it is categorical and incompatible with mathematical operations like matrix multiplication. To address this, text must be converted into continuous-valued vector representations through a process called embedding. Embeddings transform discrete tokens (words or subwords) into dense numerical vectors that capture semantic relationships, enabling neural networks to process and learn from text. Different data types (e.g., text, audio, video) require specialized embedding models. Techniques like tokenization (e.g., byte pair encoding) and positional encoding further refine these embeddings to preserve word order and context. This preprocessing step is essential for training models to understand and generate human-like text.  

<img width="662" height="402" alt="image" src="https://github.com/user-attachments/assets/f7c90f63-d33c-4264-a941-52d1cfb7b6a6" />

Several algorithms have been developed to create word embeddings, with **Word2Vec** being one of the earliest and most influential. Word2Vec trains a neural network to generate embeddings. The key insight is that words appearing in similar contexts share related meanings, allowing their embeddings to capture semantic relationships. While pretrained models like Word2Vec provide general-purpose embeddings, modern large language models (LLMs) typically generate their own embeddings as part of the input layer, which are fine-tuned during training. This approach offers a key advantage: embeddings are optimized specifically for the model's task and dataset, leading to better performance compared to static, pretrained embeddings like Word2Vec.  

Word embeddings can vary in dimensionality, ranging from just a few dimensions to thousands. Higher-dimensional embeddings (e.g., hundreds or thousands of features) can capture more nuanced semantic relationships between words, enabling models to discern subtle contextual differences. However, this increased expressiveness comes at a computational cost—larger embeddings require more memory and processing power, potentially slowing down training and inference. Lower-dimensional embeddings, while less precise, offer greater efficiency. The choice of dimensionality involves a trade-off: models like GPT-3 use high-dimensional embeddings (e.g., 12,288 dimensions) for advanced performance, whereas simpler applications may prioritize computational efficiency with smaller embeddings. Balancing expressiveness and efficiency is key in designing effective NLP systems.

##  Text Tokenization
Tokenization is the fundamental process of breaking down input text into smaller units called tokens, which serve as the basic building blocks for LLMs. These tokens can represent individual words, subwords, or special characters (including punctuation). Tokenization transforms raw text into a structured sequence that can be converted into numerical embeddings for model processing. For example, the sentence "Hello, world!" might be tokenized into ["Hello", ",", "world", "!"]. Effective tokenization is crucial. It balances granularity (preserving meaning) and efficiency (managing vocabulary size), with methods ranging from simple whitespace splitting to advanced algorithms like byte pair encoding (BPE) used in models like GPT. This preprocessing step directly impacts an LLM's ability to interpret and generate human-like text.  

<img width="563" height="628" alt="image" src="https://github.com/user-attachments/assets/1f3cc79a-45f9-487e-a307-866ef6dce8c0" />

## Mapping tokens into token IDs
After tokenizing text into individual units (words, subwords, or symbols), the next step is converting these tokens into numerical token IDs which is a prerequisite for generating embedding vectors. This process requires building a vocabulary that maps each unique token to a distinct integer identifier. For instance, in a simple vocabulary, "cat" might map to 42, while "," maps to 3. This numerical representation enables mathematical operations in neural networks. The vocabulary is typically constructed by assigning IDs to tokens based on frequency or alphabetical order, ensuring consistent encoding across the dataset. Token IDs serve as indices to retrieve corresponding embedding vectors during model processing, bridging raw text to computable data for LLMs.  

<img width="832" height="624" alt="image" src="https://github.com/user-attachments/assets/35d27090-b33e-4c55-a518-a1c66362a4a6" />

The vocabulary dictionary maps each token to a unique integer identifier. Next, we'll use this mapping to transform raw text into numerical token IDs.  

<img width="855" height="605" alt="image" src="https://github.com/user-attachments/assets/f23849ae-d1e6-44f7-922e-2e59fbca9c80" />

## Special Tokens  

When building your own text processing system, you need to plan for special cases in the vocabulary (your word dictionary). Two important special tokens to include are:
1. **<|unk|>** - This acts as a "placeholder" for words your system hasn't seen before. Like when you encounter a new word not in your dictionary, you use this instead.
2. **<|endoftext|>** - This works like a "chapter break" to separate different documents or conversations. It tells the system when one piece of text ends and another begins.

<img width="903" height="579" alt="image" src="https://github.com/user-attachments/assets/32bc52d0-b668-45f4-87a1-2e643308d1f4" />

## Byte Pair Encoding (BPE)
The **Byte Pair Encoding (BPE)** method gives models like GPT a clever way to process all words, even unfamiliar ones. Here's how it works:
1. **Special Token Notes**  
   * The **<|endoftext|>** marker (ID 50256) acts like a "stop sign" between documents.
   * Unlike simpler systems, BPE doesn't need an **<|unk|>** placeholder.
2. **The Magic of Subwords**  
   * When BPE sees an unknown word (like "someunknownPlace"), it doesn't give up
   * Instead, it breaks the word into smaller pieces it does know:
     * Example: "some" + "unknown" + "Place"
     * Or even individual letters if needed. for instance, if no subwords match, uses individual letters:
       * Example: "Zqx" → "Z" + "q" + "x"
3. **Vocabulary System**
   * Has 50,257 word "slots" in its dictionary
   * Common words get their own slot (e.g., "the")
   * Rare words are built from smaller pieces

BPE gives AI models much better language flexibility compared to systems that just mark unknown words as <|unk|>. It's why GPT models can understand specialized terms, creative spellings, and evolving vocabulary so effectively.
     
<img width="687" height="294" alt="image" src="https://github.com/user-attachments/assets/1f8e6890-0d48-415f-825d-681e2af258a0" />


## Data Sampling with Sliding Window for LLM Training

Sliding windows split text into overlapping chunks. Each input is a sequence, and the target is the next word. Sliding window helps LLMs learn language patterns efficiently. This method is fundamental in training models like ChatGPT. The sliding window is a fundamental tool for breaking long sequences into manageable, context-rich chunks. When training a Large Language Model (LLM), we need input–target pairs—sequences of text where:

1. **Input:** A partial sentence (context window).
2. **Target:** The next word (or token) the model should predict.

This is done using a sliding window approach. Step-by-Step explanation of sliding window is as follow:  

1. **Sliding Over Text**
   * The text is split into chunks (tokens/words).
   * A fixed-size window moves over the text, creating overlapping sequences.
2. **Creating Input–Target Pairs:** For each window position:
   * Input: A sequence of $n$ tokens.
   * Target: The next token after the input.

**Example:**  

<img width="595" height="364" alt="image" src="https://github.com/user-attachments/assets/851849cb-ccba-4b01-b541-98e6c6402e83" />


**Why Use a Sliding Window?**  
1. **Efficient Training:**
   * Reuses text data in overlapping windows.
   * Maximizes learning from limited data.
2. **Teaches Context Prediction:**
   * The model learns to predict the next word based on previous words.
   * Helps in autoregressive generation (like ChatGPT).
3. **Flexible Window Size:**
   * Can adjust n (e.g., n=64, n=128) based on model needs.

***Implementation***  

Before we can convert tokens into numerical embeddings for a language model, we first need to structure our data for efficient processing. This is where a Data Loader comes in. The Data Loader's job is to split the text into meaningful chunks of input-target pairs, where the input is a sequence of tokens and the target is the next token the model should predict. A tokenizer automatically handles the conversion from words to numerical token IDs, allowing the Data Loader to work with these numbers directly rather than raw text. By organizing the data into batches, the Data Loader significantly speeds up training, as the model can process multiple input-target pairs simultaneously rather than one at a time. Importantly, both the inputs and targets are represented as token IDs, which are numerical representations of the text that the model can understand and process.

To prepare the data for the Data Loader, we first create a custom class that inherits from PyTorch’s Dataset class. This class is responsible for taking the raw text, splitting it into input-target pairs (e.g., an input sequence and its corresponding next token), and providing these pairs one at a time when requested. The Dataset class has two key methods: ```__len__```, which tells us how many data pairs exist in the dataset, and ```__getitem__```, which retrieves a specific pair by its index. For example, if our dataset contains 1,000 input-target pairs, ```__len__``` returns 1,000, while ```__getitem__(5)``` fetches the 6th pair (since indexing starts at 0). This structured approach ensures the Data Loader can efficiently access and batch the data, streamlining the training process for the language model. The following picture shows the input-target pairs in LLM.

<img width="765" height="405" alt="image" src="https://github.com/user-attachments/assets/7109ebb3-6e65-44be-be99-537ac0e42347" />

**Some important concepts in sliding window**  

1. **Context-Size (window-size) in LLMs:** Context size (also called context window) is the maximum number of tokens (words/subwords) a language model can consider at once when generating text.
2. **Input-Target pairs:** Input-target pairs are the building blocks for training language models like GPT. They teach the model to predict the next word (or token) in a sequence.
   * Input: A chunk of text (context)
   * Target: The next word/token that should follow
3. **Stride:** Stride determines how much the sliding window moves forward when creating input-target pairs from text. It controls the overlap between consecutive sequences during training.
4. **Sliding-window:** A sliding window is a technique that processes sequential data (like text) by moving a fixed-size "window" across the input to extract smaller, overlapping chunks for analysis or training. Its key characteristics:
   * Fixed Window Size (max_length): Determines how many tokens/words are visible at once (e.g., 4 words).
   * Stride (step): Controls how far the window moves forward each step (e.g., 1 word = max overlap and if stride = window size no overlap).
   * Balances Context & Efficiency:
     * Small windows → Faster training but limited context.
     * Large windows → Better context but higher memory use.

## Creating Token Embeddings for LLMs  

Embeddings transform discrete tokens into continuous vectors. To prepare tokenized text for training, we convert token IDs into embedding vectors—numerical representations that capture semantic meaning. This step bridges raw text to the mathematical world of deep learning. For example:
* "cat" (ID: 42) → [0.3, -0.8, 1.2]
* "dog" (ID: 17) → [0.4, -0.7, 1.1]
  
At start, each word gets assigned a unique list of random numbers (like [0.2, -1.7, 0.4...]). These number lists are called "embedding vectors".  

<img width="714" height="572" alt="image" src="https://github.com/user-attachments/assets/db651025-0abc-467c-9c38-bd9b397e0329" />

For GPT-style language models to process text, we first convert discrete tokens into continuous vector representations called embeddings. This transformation is crucial because these models are deep neural networks that learn through backpropagation, which requires numerical inputs. Unlike simple word indexes, embedding vectors allow the model to represent words in a high-dimensional space where semantic relationships can be captured through relative positions and distances. The continuous nature of these vectors enables backpropagation to make fine-grained adjustments during training, gradually refining the model's understanding of linguistic patterns and word associations. This vector transformation from discrete tokens to continuous space is what allows neural networks to effectively process and generate human-like text.  

In our example with 6 tokens, we create a 3-dimensional embedding space. The embedding layer contains a weight matrix of small random values (typically initialized with values between -0.1 and 0.1), with dimensions 6×3 - one row per token and one column per embedding dimension. During training, this matrix gets optimized alongside all other model parameters. For instance, if our vocabulary contains tokens ["the", "cat", "sat", "on", "mat", "."], each would be mapped to a unique 3D vector like [0.12, -0.05, 0.08], allowing the model to learn meaningful relationships between words through their relative positions in this continuous space. For another exampe:

1. Vocabulary: ["apple", "banana", "orange", "run", "jump", "the"]
2. Token IDs: $[0, 1, 2, 3, 4, 5]$
3. Embedding dimension: 3
4. Initial Embedding Matrix $(6×3)$:
   ```
    embedding_weights = [
         [ 0.12, -0.45,  0.23],  # Token 0 ("apple")
         [-0.67,  0.34,  0.89],  # Token 1 ("banana")
         [ 0.55, -0.12,  0.76],  # Token 2 ("orange")
         [-0.32,  0.91, -0.44],  # Token 3 ("run")
         [ 0.78,  0.01,  0.33],  # Token 4 ("jump")
         [-0.09, -0.56, -0.21]   # Token 5 ("the")
      ]
   ```
   * The word "banana" (Token ID=1) becomes the vector [-0.67, 0.34, 0.89]
   * The word "jump" (Token ID=4) becomes [0.78, 0.01, 0.33]
  
The embedding layer in a language model functions like a **lookup table**, converting token IDs into their corresponding embedding vectors stored in a weight matrix. Since Python uses zero-based indexing, token ID 5 retrieves the 6th row from this matrix. For example, with our small vocabulary of 6 words, each token ID (0-5) maps directly to a specific row in the matrix - ID 0 accesses the first row, ID 1 the second row, and so on. These embeddings, initially containing small random values, will later be optimized during training to encode meaningful semantic relationships between words. The lookup process is efficient, simply selecting the pre-computed vector associated with each token's assigned numerical ID.  

By the consideration of previous embedding weight matrix $(6×3)$, for an example the **embedding lookup process** will be as follow:
1. When token ID 5 ("the") appears in text:
   * The embedding layer looks up Row 5 (6th row).
   * Retrieves the vector: $[-0.09, -0.56, -0.21]$.
   * This becomes the input for the neural network.
2. The embedding layer acts like a dictionary: **token ID → vector**.
3. **Similar words get similar vectors.**
4. During training, these vectors evolve. In another words, the weight matrix is just a big table that gets **optimized during training**.

<img width="675" height="243" alt="image" src="https://github.com/user-attachments/assets/d8a0a372-534a-44a9-8568-cef95ec0f448" />

## Position-Aware Embeddings for LLMs

While token embeddings effectively represent word meanings, they lack crucial positional information - the same word always receives identical representation regardless of its location in a sentence. This creates a significant limitation because language meaning depends fundamentally on word order (compare "dog bites man" vs. "man bites dog").

<img width="702" height="397" alt="image" src="https://github.com/user-attachments/assets/7dcd822c-35eb-450f-9936-0335b02a31c0" />

Since the self-attention mechanism in LLMs is inherently position-agnostic (it treats words equally regardless of order), we must explicitly encode where words appear in a sequence. To solve this, modern LLMs incorporate positional encoding, which:
1. Generates unique position vectors for each possible token position
2. Combines these with token embeddings before processing
3. Preserves both semantic meaning and sequential order

Positional encoding for example can handle the following sentence: 
* **The model can distinguish "I saw the saw" (where identical words play different grammatical roles)**

**Two Approaches to Positional Encoding:**
1. **Absolute Positional Embeddings:** Assigns a fixed, unique embedding to each position in the sequence. These embeddings are added to the token embeddings before processing. This ensures the model knows exact word positions (e.g., "bank" at the start vs. end of a sentence). For Example:
   * Position 1 → [0.1, -0.3, 0.5]
   * Position 2 → [-0.2, 0.4, 0.1
  
     <img width="835" height="304" alt="image" src="https://github.com/user-attachments/assets/3ae4f806-2c3a-4de8-a85b-5b0cc3fa2f8d" />
   
2. **Relative Positional Embeddings:** Encodes the distance between words rather than fixed positions. It helps the model learn flexible patterns (e.g., verbs often follow nouns). For example, for token Sequence: [The, cat, sat, on, the, mat], assume positions are: [0, 1, 2, 3, 4, 5]. How Relative Embeddings Work:  
   * **Focus on Token Pairs:** For each token (e.g., "sat" at position 2), the model considers its distance to other tokens:
     * "cat" is 1 position left (offset = -1)
     * "the" (second occurrence) is 2 positions right (offset = +2)
   * **Embedding Lookup by Offset:**
     * Example embeddings for common offsets:
     ```
       Offset -2: [0.1, -0.3, 0.2]  
       Offset -1: [0.3, 0.0, -0.1]  
       Offset  0: [0.5, 0.5, 0.5]  (same token)  
       Offset +1: [-0.1, 0.2, 0.4]  
       Offset +2: [-0.3, 0.1, 0.3]
     ```
   * **Applied to Self-Attention:** When processing "sat", the model:
     * Uses offset -1’s embedding to relate "sat" to "cat"
     * Uses offset +2’s embedding to relate "sat" to "the"
