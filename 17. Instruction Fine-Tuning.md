# Instruction Fine-Tuning
This section introduces the core concept of instruction fine-tuning. Pretrained LLMs (like base GPT-2) are excellent at text completion but poor at instruction following:
* Can complete sentences: "The weather today is..." → "sunny and warm"
* Struggle with instructions: "Fix this grammar: 'He go to school'" → Often continues incorrectly

Instruction fine-tuning (also called supervised instruction fine-tuning) transforms a pretrained LLM into one that can:
* Understand specific instructions
* Generate appropriate responses (grammar correction, text transformation, Q&A, etc.)

<img width="668" height="378" alt="image" src="https://github.com/user-attachments/assets/f4ad5893-6546-4c60-93f0-27cbe22f12e4" />

**Supervised instruction fine-tuning follows three-stage process**  

**Stage 1: Dataset Preparation**
* Step 1: Download and format the instruction dataset
* Step 2: Organize data into training and target batches

**Stage 2: Model Setup & Fine-tuning**
* Step 3: Load a pretrained LLM
* Step 4: Fine-tune on instruction data

**Stage 3: Evaluation**
* Step 5: Extract and evaluate model responses
* Step 6: Quantify performance

<img width="616" height="514" alt="image" src="https://github.com/user-attachments/assets/072c7abf-4a22-4082-a7c5-0fdcb1ac0aff" />

## Dataset preparation
The choice of prompt style is crucial because it teaches the model the conversation structure it should follow. During instruction fine-tuning, the model learns:
* **To recognize instruction patterns:** How instructions are presented
* **When to generate responses:** The transition from input to output
* **Response formatting:** How to structure its answers appropriately

The following example show a dataset with instruction-response pairs in JSON format structure:
```
{
    'instruction': 'What is an antonym of complicated?',
    'input': '',  # Can be empty
    'output': 'An antonym of complicated is simple.'
}
```

### Alpaca Style 
Alpaca is one of the first publicly documented instruction fine-tuning approaches. It has clear structure that explicitly make the training objective very clear to the model. The alpaca is widely adopted in the open-source LLM community. Below shows the Alpaca style:
```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
[Instruction text]

### Input:
[Input text]  # Optional section

### Response:
[Expected output]
```

### Phi-3 Style
The Phi-3 style uses special tokens instead of verbose section headers. It mimics chat-based interfaces more closely and less overhead in the prompt. Below shows the Phi-3 style:
```
<|user|>
[Instruction text]
[Input text]  # If provided
<|assistant|>
[Expected output]
```
This figure is a great comparison of the two main prompt styling approaches for instruction fine-tuning.

<img width="709" height="444" alt="image" src="https://github.com/user-attachments/assets/259bb4a7-8002-4b52-b73b-c70198bc4de6" />

### Alpaca Style is Chosen for This Chapter
Both styles teach the model the same fundamental concept: recognize when it's being given an instruction and generate an appropriate response. The main difference is in how that context is presented. Different styles work better for different use cases, which is why both remain popular in the LLM ecosystem. The choice often depends on:
1. Target deployment scenario (chat vs. instruction-following)
2. Model size and capacity (larger models can handle more complex formatting)
3. Existing infrastructure (compatibility with current systems)

In this chapter, Alpaca style is used for the following reasons:

1. **Educational Value:** The explicit structure makes it easier to understand what's happening during training
2. **Debugging Friendly:** Clear section boundaries help identify issues
3. **Proven Approach:** Well-established method with documented success
4. **Transparency:** Easy to see exactly what the model is learning
