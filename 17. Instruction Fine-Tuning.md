# Instruction Fine-Tuning
This section introduces the core concept of instruction fine-tuning. Pretrained LLMs (like base GPT-2) are excellent at text completion but poor at instruction following:
* Can complete sentences: "The weather today is..." → "sunny and warm"
* Struggle with instructions: "Fix this grammar: 'He go to school'" → Often continues incorrectly

Instruction fine-tuning (also called supervised instruction fine-tuning) transforms a pretrained LLM into one that can:
* Understand specific instructions
* Generate appropriate responses (grammar correction, text transformation, Q&A, etc.)

<img width="668" height="378" alt="image" src="https://github.com/user-attachments/assets/f4ad5893-6546-4c60-93f0-27cbe22f12e4" />

**Supervised instruction fine-tuning follows three-stage process**  

**Stage 1: Dataset Preparation**
* Step 1: Download and format the instruction dataset
* Step 2: Organize data into training and target batches

**Stage 2: Model Setup & Fine-tuning**
* Step 3: Load a pretrained LLM
* Step 4: Fine-tune on instruction data

**Stage 3: Evaluation**
* Step 5: Extract and evaluate model responses
* Step 6: Quantify performance

<img width="616" height="514" alt="image" src="https://github.com/user-attachments/assets/072c7abf-4a22-4082-a7c5-0fdcb1ac0aff" />

## Dataset preparation
The choice of prompt style is crucial because it teaches the model the conversation structure it should follow. During instruction fine-tuning, the model learns:
* **To recognize instruction patterns:** How instructions are presented
* **When to generate responses:** The transition from input to output
* **Response formatting:** How to structure its answers appropriately

The following example show a dataset with instruction-response pairs in JSON format structure:
```
{
    'instruction': 'What is an antonym of complicated?',
    'input': '',  # Can be empty
    'output': 'An antonym of complicated is simple.'
}
```

### Alpaca Style 
Alpaca is one of the first publicly documented instruction fine-tuning approaches. It has clear structure that explicitly make the training objective very clear to the model. The alpaca is widely adopted in the open-source LLM community. Below shows the Alpaca style:
```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
[Instruction text]

### Input:
[Input text]  # Optional section

### Response:
[Expected output]
```

### Phi-3 Style
The Phi-3 style uses special tokens instead of verbose section headers. It mimics chat-based interfaces more closely and less overhead in the prompt. Below shows the Phi-3 style:
```
<|user|>
[Instruction text]
[Input text]  # If provided
<|assistant|>
[Expected output]
```
This figure is a great comparison of the two main prompt styling approaches for instruction fine-tuning.

<img width="709" height="444" alt="image" src="https://github.com/user-attachments/assets/259bb4a7-8002-4b52-b73b-c70198bc4de6" />

### Alpaca Style is Chosen for This Chapter
Both styles teach the model the same fundamental concept: recognize when it's being given an instruction and generate an appropriate response. The main difference is in how that context is presented. Different styles work better for different use cases, which is why both remain popular in the LLM ecosystem. The choice often depends on:
1. Target deployment scenario (chat vs. instruction-following)
2. Model size and capacity (larger models can handle more complex formatting)
3. Existing infrastructure (compatibility with current systems)

In this chapter, Alpaca style is used for the following reasons:

1. **Educational Value:** The explicit structure makes it easier to understand what's happening during training
2. **Debugging Friendly:** Clear section boundaries help identify issues
3. **Proven Approach:** Well-established method with documented success
4. **Transparency:** Easy to see exactly what the model is learning

### Custom Batching
This section covers the critical batching process for instruction fine-tuning, which requires special handling compared to standard classification tasks. Instruction fine-tuning has unique requirements that standard PyTorch DataLoader can't handle automatically. The 5-Step Batching Process:
1. **Step 1: Applying Prompt Template**
   * Convert raw dictionary entries into formatted text using Alpaca style
   * Creates the full training text that the model will see
2. **Step 2: Tokenization**
   * Convert formatted text into token IDs using GPT-2 tokenizer
   * Same tokenization approach used in pretraining
3. **Step 3: Adding Padding Tokens**
   * Pad sequences to the longest example in each batch (not entire dataset)
   * Uses ```<|endoftext|>``` token (**ID 50256**) for padding
   * Enables efficient GPU processing with uniform tensor shapes
4. **Step 4: Creating Target Token IDs**
   * Shift input tokens one position to the right for next-token prediction
   * Standard autoregressive training approach used in pretraining
5. **Step 5: Masking Padding Tokens**
   * Replace padding tokens in targets with -100 (PyTorch's ignore_index)
   * Prevents padding tokens from contributing to loss calculation
   * Crucial for training efficiency and accuracy

<img width="614" height="592" alt="image" src="https://github.com/user-attachments/assets/fdc4e362-5b8b-49fe-b934-c87e02765478" />

Figure below illustrates the crucial data preprocessing pipeline that transforms raw instruction data into model-ready token sequences. Let me break down these two foundational steps:

<img width="705" height="472" alt="image" src="https://github.com/user-attachments/assets/affff1bc-6f94-4a23-a25a-23420df955ea" />


### Dynamic Batch Padding
This section introduces a smart padding strategy that's crucial for efficient instruction fine-tuning. If we padded all sequences to the dataset maximum length:
* Most sequences would have excessive padding
* Wasted computation on padding tokens
* Slower training due to processing unnecessary tokens
* Memory inefficiency

**The Solution: Dynamic Batch Padding** Pad sequences only to the longest sequence in each batch, not the entire dataset. Benefits of dynamic batch padding:
1. **Computational Efficiency**
   * Less padding: Only necessary padding per batch
   * Faster training: Shorter sequences when possible
   * GPU memory optimization: Reduced memory footprint
2. **Training Quality**
   * More meaningful tokens: Higher ratio of real data vs padding
   * Better gradient updates: Loss calculated on actual content
   * Reduced overfitting: Less exposure to repetitive padding patterns
3. **Flexibility**
   * Handles variability: Accommodates diverse instruction lengths
   * Adaptive: Each batch optimized for its specific content
   * Scalable: Works with datasets of any size distribution

**Visual Example from Figure 7.8**

1. **Batch 1 (Long sequences - more padding):**
```
Input 1: [Token11, Token12, Token13, Token14, Token15]
Input 2: [Token21, Token22, <PAD>, <PAD>, <PAD>]
Input 3: [Token31, Token32, Token33, <PAD>, <PAD>]
```
2. **Batch 2 (Short sequences - less padding):**
```
Input 4: [Token41, Token42]
Input 5: [Token51, Token52, Token53]  
Input 6: [Token61, <PAD>, <PAD>]  # Padded to length 3
```

### Target sequence
Target sequence is generated by "shift right by one" operation which is the fundamental mechanism that enables next-token prediction. The core idea is train the model to predict the next token given all previous tokens. Example:

**Input Sequence (Context):**

```
Tokens:  [A,    B,    C,    D,    E]
```

**Target Sequence (What to predict):**

```
Targets: [B,    C,    D,    E,    <EOS>]
```

**Training Process:**

```
Given A → Predict B
Given A, B → Predict C
Given A, B, C → Predict D
Given A, B, C, D → Predict E
Given A, B, C, D, E → Predict <EOS>
```

The above example, highlights the critical step of target sequence preparation. the mechanism that actually teaches the model what to generate. During inference, the model generates text token-by-token **(Autoregressive Generation)**:
* Start with initial prompt
* Predict next token
* Append predicted token to context
* Repeat until <EOS> is generated

This step is essential, because without proper targets, the model doesn't know what to predict.

### Selective Masking with -100
This is a crucial optimization step that significantly improves training efficiency and quality. When we pad sequences to make them the same length within a batch, we create artificial tokens that shouldn't contribute to learning:
```
Input:   [A, B, C, <PAD>, <PAD>]
Target:  [B, C, <PAD>, <PAD>, <PAD>]  # All padding tokens affect loss!
```
This would teach the model to predict padding tokens, which is useless. The solution is using selective masking with -100. So, why -100? Because PyTorch's ```cross_entropy_loss(ignore_index=-100)``` automatically excludes these positions from loss calculation. Masking strategy is keeping the first ```<EOS>``` token but mask additional padding tokens: 

**Before Masking:**
```
Target:  [B, C, <EOS>, <PAD>, <PAD>]  # From shifting right
```

**After Masking:**
```
Target:  [B, C, <EOS>, -100, -100]    # Only meaningful tokens contribute
```

**Why Keep One EOS Token?** The model needs to learn when to stop generating. By keeping one ```<EOS>``` token:
1. **Learns stop conditions:** Understands when a response is complete.
2. **Prevents infinite generation:** Knows when to end responses.
3. **Natural conversation flow:** Mimics real dialogue endings.

**Example**

**Batch with different-length sequences:**
```
Sequence 1: [A, B, C, D, E]          → Length: 5
Sequence 2: [F, G, H]                 → Length: 3  
Sequence 3: [I, J, K, L]              → Length: 4
```

**After padding to max length 6:**
```
Inputs:   [A, B, C, D, E, <PAD>]
          [F, G, H, <PAD>, <PAD>, <PAD>]
          [I, J, K, L, <PAD>, <PAD>]

Targets:  [B, C, D, E, <EOS>, -100]      # First EOS kept
          [G, H, <EOS>, -100, -100, -100]
          [J, K, L, <EOS>, -100, -100]
```

**Impact on Training Efficiency**
1. **Without Masking:**
   * Loss calculated on all padding tokens.
   * Wasted computation (~30-50% of batches could be padding).
   * Slower convergence.
   * Poorer quality learning.
2. **With Masking:**
   * Loss only on meaningful tokens.
   * Faster training (less computation).
   * Better gradient updates.
   * Focused learning on actual content

**Difference from Classification Fine-Tuning**  
In classification, we only cared about the last token's prediction, so padding didn't affect the loss. Here, we care about every token in the sequence, so proper masking is essential.

**The PyTorch Cross-Entropy Convention**  
The ignore_index parameter tells the loss function: "When you see target value -100, skip calculating loss for that position."
```
torch.nn.functional.cross_entropy(..., ignore_index=-100)
```
The -100 convention is a great example of how thoughtful API design in deep learning frameworks enables efficient and effective model training. It's a simple mechanism that solves a very practical problem in elegant way.

### What is Instruction Masking?
Instruction masking means applying the -100 ignore index to target tokens that correspond to the instruction portion of the text, **so loss is only calculated on the response tokens**. The following example shows **without instruction masking** (Current Approach):
```
Full text: "### Instruction: Fix grammar ### Input: He go to school ### Response: He goes to school"

Input:  [###, Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :]
Target: [Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :, He, goes, to, school]
# Loss calculated on ALL tokens (instruction + response)
```

The following example show **with instruction masking** (alternative approach):
```
Input:  [###, Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :]
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, He, goes, to, school]
# Loss only calculated on response tokens
```

**Reasons why Instruction Masking was suggested?**
1. **Focus on Response Generation**
   * **Primary goal:** Teach the model to generate good responses, not reconstruct instructions.
   * **Targeted learning:** Concentrates model capacity on what matters most.
   * **Clear objective:** "Only train on what you want the model to generate"
2. **Computational Efficiency**
   * **Fewer loss calculations:** Reduces unnecessary computations.
   * **Faster training:** Less backward pass through irrelevant tokens.
   * **Memory savings:** Smaller effective sequence lengths.
3. **Avoid Instruction Pattern Memorization**
   * **Prevent memorization:** Avoids learning specific instruction phrasing patterns
   * **Efficient learning:** Computational resources focused on valuable targets
   * **Better generalization:** Model learns response patterns that work across different instruction wordings
  
**The Emerging Evidence AGAINST Instruction Masking**

Despite these reasons, recent research (like Shi et al. 2024) suggests that the benefits of instruction masking may be smaller than previously thought, and in some cases, no masking can produce better results. The paper "Instruction Tuning With Loss Over Instructions" found:
1. No consistent benefits from instruction masking across various models.
2. Sometimes worse performance when masking instructions.
3. Better generalization when models learn full instruction+response patterns.
4. Holistic understanding improves with unmasked training.
5. **Suggestion:** Calculate loss over both instructions and responses for better results

**For example**  

**Input and Target Preparation - WITHOUT Instruction Masking**
```
Input:  [100, 101, 200, 201, 202, 300, 301, 400, 401, 500, 501]
Target: [101, 200, 201, 202, 300, 301, 400, 401, 500, 501, 600, 601, 602, 50256]
```
**Loss is calculated on ALL target positions:**  
```
Target: [101, 200, 201, 202, 300, 301, 400, 401, 500, 501, 600, 601, 602, 50256]
         ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑
        All these positions contribute to loss (instruction + response)
```

**Input and Target Preparation - WITH Instruction Masking**
```
Input:  [100, 101, 200, 201, 202, 300, 301, 400, 401, 500, 501]
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 600, 601, 602, 50256]
```
**Loss Calculation:**
```
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 600, 601, 602, 50256]
         ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑    ↑    ↑    ↑
                Ignored                       Only these contribute to loss (response only)
```

**This chapter chooses no masking for the following reasons:**  

1. **Simpler implementation:** Fewer moving parts for educational code.
2. **Established practice:** Many successful instruction-tuned models don't use instruction masking.
3. **Research-backed:** Recent evidence suggests it might not be beneficial.
4. **Clearer learning progression:** Students can see the full training process
