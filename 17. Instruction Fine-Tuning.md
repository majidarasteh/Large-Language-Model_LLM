# Instruction Fine-Tuning
This section introduces the core concept of instruction fine-tuning. Pretrained LLMs (like base GPT-2) are excellent at text completion but poor at instruction following:
* Can complete sentences: "The weather today is..." → "sunny and warm"
* Struggle with instructions: "Fix this grammar: 'He go to school'" → Often continues incorrectly

Instruction fine-tuning (also called supervised instruction fine-tuning) transforms a pretrained LLM into one that can:
* Understand specific instructions
* Generate appropriate responses (grammar correction, text transformation, Q&A, etc.)

<img width="668" height="378" alt="image" src="https://github.com/user-attachments/assets/f4ad5893-6546-4c60-93f0-27cbe22f12e4" />

**Supervised instruction fine-tuning follows three-stage process**  

**Stage 1: Dataset Preparation**
* Step 1: Download and format the instruction dataset
* Step 2: Organize data into training and target batches

**Stage 2: Model Setup & Fine-tuning**
* Step 3: Load a pretrained LLM
* Step 4: Fine-tune on instruction data

**Stage 3: Evaluation**
* Step 5: Extract and evaluate model responses
* Step 6: Quantify performance

<img width="660" height="414" alt="image" src="https://github.com/user-attachments/assets/ca09a487-8a92-4e82-9706-861dfe13d2e5" />

## Dataset preparation
The choice of prompt style is crucial because it teaches the model the conversation structure it should follow. During instruction fine-tuning, the model learns:
* **To recognize instruction patterns:** How instructions are presented
* **When to generate responses:** The transition from input to output
* **Response formatting:** How to structure its answers appropriately

The following example show a dataset with instruction-response pairs in JSON format structure:
```
{
    'instruction': 'What is an antonym of complicated?',
    'input': '',  # Can be empty
    'output': 'An antonym of complicated is simple.'
}
```

### Alpaca Style 
Alpaca is one of the first publicly documented instruction fine-tuning approaches. It has clear structure that explicitly make the training objective very clear to the model. The alpaca is widely adopted in the open-source LLM community. Below shows the Alpaca style:
```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
[Instruction text]

### Input:
[Input text]  # Optional section

### Response:
[Expected output]
```

### Phi-3 Style
The Phi-3 style uses special tokens instead of verbose section headers. It mimics chat-based interfaces more closely and less overhead in the prompt. Below shows the Phi-3 style:
```
<|user|>
[Instruction text]
[Input text]  # If provided
<|assistant|>
[Expected output]
```
This figure is a great comparison of the two main prompt styling approaches for instruction fine-tuning.

<img width="709" height="444" alt="image" src="https://github.com/user-attachments/assets/259bb4a7-8002-4b52-b73b-c70198bc4de6" />

### Alpaca Style is Chosen for This Chapter
Both styles teach the model the same fundamental concept: recognize when it's being given an instruction and generate an appropriate response. The main difference is in how that context is presented. Different styles work better for different use cases, which is why both remain popular in the LLM ecosystem. The choice often depends on:
1. Target deployment scenario (chat vs. instruction-following)
2. Model size and capacity (larger models can handle more complex formatting)
3. Existing infrastructure (compatibility with current systems)

In this chapter, Alpaca style is used for the following reasons:

1. **Educational Value:** The explicit structure makes it easier to understand what's happening during training
2. **Debugging Friendly:** Clear section boundaries help identify issues
3. **Proven Approach:** Well-established method with documented success
4. **Transparency:** Easy to see exactly what the model is learning

### Custom Batching
This section covers the critical batching process for instruction fine-tuning, which requires special handling compared to standard classification tasks. Instruction fine-tuning has unique requirements that standard PyTorch DataLoader can't handle automatically. The 5-Step Batching Process:
1. **Step 1: Applying Prompt Template**
   * Convert raw dictionary entries into formatted text using Alpaca style
   * Creates the full training text that the model will see
2. **Step 2: Tokenization**
   * Convert formatted text into token IDs using GPT-2 tokenizer
   * Same tokenization approach used in pretraining
3. **Step 3: Adding Padding Tokens**
   * Pad sequences to the longest example in each batch (not entire dataset)
   * Uses ```<|endoftext|>``` token (**ID 50256**) for padding
   * Enables efficient GPU processing with uniform tensor shapes
4. **Step 4: Creating Target Token IDs**
   * Shift input tokens one position to the right for next-token prediction
   * Standard autoregressive training approach used in pretraining
5. **Step 5: Masking Padding Tokens**
   * Replace padding tokens in targets with -100 (PyTorch's ignore_index)
   * Prevents padding tokens from contributing to loss calculation
   * Crucial for training efficiency and accuracy

<img width="614" height="592" alt="image" src="https://github.com/user-attachments/assets/fdc4e362-5b8b-49fe-b934-c87e02765478" />

Figure below illustrates the crucial data preprocessing pipeline that transforms raw instruction data into model-ready token sequences. Let me break down these two foundational steps:

<img width="705" height="472" alt="image" src="https://github.com/user-attachments/assets/affff1bc-6f94-4a23-a25a-23420df955ea" />


### Dynamic Batch Padding
This section introduces a smart padding strategy that's crucial for efficient instruction fine-tuning. If we padded all sequences to the dataset maximum length:
* Most sequences would have excessive padding
* Wasted computation on padding tokens
* Slower training due to processing unnecessary tokens
* Memory inefficiency

**The Solution: Dynamic Batch Padding** Pad sequences only to the longest sequence in each batch, not the entire dataset. Benefits of dynamic batch padding:
1. **Computational Efficiency**
   * Less padding: Only necessary padding per batch
   * Faster training: Shorter sequences when possible
   * GPU memory optimization: Reduced memory footprint
2. **Training Quality**
   * More meaningful tokens: Higher ratio of real data vs padding
   * Better gradient updates: Loss calculated on actual content
   * Reduced overfitting: Less exposure to repetitive padding patterns
3. **Flexibility**
   * Handles variability: Accommodates diverse instruction lengths
   * Adaptive: Each batch optimized for its specific content
   * Scalable: Works with datasets of any size distribution

**Visual Example from Figure 7.8**

1. **Batch 1 (Long sequences - more padding):**
```
Input 1: [Token11, Token12, Token13, Token14, Token15]
Input 2: [Token21, Token22, <PAD>, <PAD>, <PAD>]
Input 3: [Token31, Token32, Token33, <PAD>, <PAD>]
```
2. **Batch 2 (Short sequences - less padding):**
```
Input 4: [Token41, Token42]
Input 5: [Token51, Token52, Token53]  
Input 6: [Token61, <PAD>, <PAD>]  # Padded to length 3
```

### Target sequence
Target sequence is generated by "shift right by one" operation which is the fundamental mechanism that enables next-token prediction. The core idea is train the model to predict the next token given all previous tokens. Example:

**Input Sequence (Context):**

```
Tokens:  [A,    B,    C,    D,    E]
```

**Target Sequence (What to predict):**

```
Targets: [B,    C,    D,    E,    <EOS>]
```

**Training Process:**

```
Given A → Predict B
Given A, B → Predict C
Given A, B, C → Predict D
Given A, B, C, D → Predict E
Given A, B, C, D, E → Predict <EOS>
```

The above example, highlights the critical step of target sequence preparation. the mechanism that actually teaches the model what to generate. During inference, the model generates text token-by-token **(Autoregressive Generation)**:
* Start with initial prompt
* Predict next token
* Append predicted token to context
* Repeat until <EOS> is generated

This step is essential, because without proper targets, the model doesn't know what to predict.

### Selective Masking with -100
This is a crucial optimization step that significantly improves training efficiency and quality. When we pad sequences to make them the same length within a batch, we create artificial tokens that shouldn't contribute to learning:
```
Input:   [A, B, C, <PAD>, <PAD>]
Target:  [B, C, <PAD>, <PAD>, <PAD>]  # All padding tokens affect loss!
```
This would teach the model to predict padding tokens, which is useless. The solution is using selective masking with -100. So, why -100? Because PyTorch's ```cross_entropy_loss(ignore_index=-100)``` automatically excludes these positions from loss calculation. Masking strategy is keeping the first ```<EOS>``` token but mask additional padding tokens: 

**Before Masking:**
```
Target:  [B, C, <EOS>, <PAD>, <PAD>]  # From shifting right
```

**After Masking:**
```
Target:  [B, C, <EOS>, -100, -100]    # Only meaningful tokens contribute
```

**Why Keep One EOS Token?** The model needs to learn when to stop generating. By keeping one ```<EOS>``` token:
1. **Learns stop conditions:** Understands when a response is complete.
2. **Prevents infinite generation:** Knows when to end responses.
3. **Natural conversation flow:** Mimics real dialogue endings.

**Example**

**Batch with different-length sequences:**
```
Sequence 1: [A, B, C, D, E]          → Length: 5
Sequence 2: [F, G, H]                 → Length: 3  
Sequence 3: [I, J, K, L]              → Length: 4
```

**After padding to max length 6:**
```
Inputs:   [A, B, C, D, E, <PAD>]
          [F, G, H, <PAD>, <PAD>, <PAD>]
          [I, J, K, L, <PAD>, <PAD>]

Targets:  [B, C, D, E, <EOS>, -100]      # First EOS kept
          [G, H, <EOS>, -100, -100, -100]
          [J, K, L, <EOS>, -100, -100]
```

**Impact on Training Efficiency**
1. **Without Masking:**
   * Loss calculated on all padding tokens.
   * Wasted computation (~30-50% of batches could be padding).
   * Slower convergence.
   * Poorer quality learning.
2. **With Masking:**
   * Loss only on meaningful tokens.
   * Faster training (less computation).
   * Better gradient updates.
   * Focused learning on actual content

**Difference from Classification Fine-Tuning**  
In classification, we only cared about the last token's prediction, so padding didn't affect the loss. Here, we care about every token in the sequence, so proper masking is essential.

**The PyTorch Cross-Entropy Convention**  
The ignore_index parameter tells the loss function: "When you see target value -100, skip calculating loss for that position."
```
torch.nn.functional.cross_entropy(..., ignore_index=-100)
```
The -100 convention is a great example of how thoughtful API design in deep learning frameworks enables efficient and effective model training. It's a simple mechanism that solves a very practical problem in elegant way.

### What is Instruction Masking?
Instruction masking means applying the -100 ignore index to target tokens that correspond to the instruction portion of the text, **so loss is only calculated on the response tokens**. The following example shows **without instruction masking** (Current Approach):
```
Full text: "### Instruction: Fix grammar ### Input: He go to school ### Response: He goes to school"

Input:  [###, Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :]
Target: [Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :, He, goes, to, school]
# Loss calculated on ALL tokens (instruction + response)
```

The following example show **with instruction masking** (alternative approach):
```
Input:  [###, Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :]
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, He, goes, to, school]
# Loss only calculated on response tokens
```

**Reasons why Instruction Masking was suggested?**
1. **Focus on Response Generation**
   * **Primary goal:** Teach the model to generate good responses, not reconstruct instructions.
   * **Targeted learning:** Concentrates model capacity on what matters most.
   * **Clear objective:** "Only train on what you want the model to generate"
2. **Computational Efficiency**
   * **Fewer loss calculations:** Reduces unnecessary computations.
   * **Faster training:** Less backward pass through irrelevant tokens.
   * **Memory savings:** Smaller effective sequence lengths.
3. **Avoid Instruction Pattern Memorization**
   * **Prevent memorization:** Avoids learning specific instruction phrasing patterns
   * **Efficient learning:** Computational resources focused on valuable targets
   * **Better generalization:** Model learns response patterns that work across different instruction wordings
  
**The Emerging Evidence AGAINST Instruction Masking**

Despite these reasons, recent research (like Shi et al. 2024) suggests that the benefits of instruction masking may be smaller than previously thought, and in some cases, no masking can produce better results. The paper "Instruction Tuning With Loss Over Instructions" found:
1. No consistent benefits from instruction masking across various models.
2. Sometimes worse performance when masking instructions.
3. Better generalization when models learn full instruction+response patterns.
4. Holistic understanding improves with unmasked training.
5. **Suggestion:** Calculate loss over both instructions and responses for better results

**For example**  

**Input and Target Preparation - WITHOUT Instruction Masking**
```
Input:  [100, 101, 200, 201, 202, 300, 301, 400, 401, 500, 501]
Target: [101, 200, 201, 202, 300, 301, 400, 401, 500, 501, 600, 601, 602, 50256]
```
**Loss is calculated on ALL target positions:**  
```
Target: [101, 200, 201, 202, 300, 301, 400, 401, 500, 501, 600, 601, 602, 50256]
         ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑
        All these positions contribute to loss (instruction + response)
```

**Input and Target Preparation - WITH Instruction Masking**
```
Input:  [100, 101, 200, 201, 202, 300, 301, 400, 401, 500, 501]
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 600, 601, 602, 50256]
```
**Loss Calculation:**
```
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 600, 601, 602, 50256]
         ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑    ↑    ↑    ↑
                Ignored                       Only these contribute to loss (response only)
```

**This chapter chooses no masking for the following reasons:**  

1. **Simpler implementation:** Fewer moving parts for educational code.
2. **Established practice:** Many successful instruction-tuned models don't use instruction masking.
3. **Research-backed:** Recent evidence suggests it might not be beneficial.
4. **Clearer learning progression:** Students can see the full training process

After loading the GPT-2 medium, we check its performance for instruction-following task. This "before" snapshot is crucial because it provides a clear baseline against which we can measure the improvements after fine-tuning, making the value of instruction tuning concrete and measurable. For example, below is an instruction that describes a task. Write a response that appropriately completes the request:
```
### Instruction:
Convert the active sentence to passive: 'The chef cooks the meal every day.'
```

**Expected response is as follow:**
```
The meal is cooked every day by the chef.
```

**Actual pretrained model response:**
```
### Response:

The chef cooks the meal every day.

### Instruction:

Convert the active sentence to passive: 'The chef cooks the
```

Analysis of pretrained model behavior show the model for the following reasons created a wrong response:
1. **Text completion mindset:** The model treats it as text continuation, not instruction execution.
2. **Pattern repetition:** Simply repeats parts of the instruction and input.
3. **No task understanding:** Doesn't comprehend the "convert to passive" request.
4. **Format confusion:** Starts new instruction sections instead of completing the response

**Why This Happens:**
* **Pretrained on general text:** Learned to complete text patterns, not follow instructions.
* **No instruction-response training:** Never saw structured instruction datasets during pretraining.
* **Autoregressive bias:** Predicts what comes next based on training data patterns.
* **Lack of specialization:** General language model, not specialized for instructions.

### The Original Alpaca Dataset
This is an excellent exercise for readers who want to scale up their instruction fine-tuning with a real-world dataset. Let me provide a comprehensive guide for working with the Alpaca dataset. **Dataset Specifications**:
* **Size**: 52,002 instruction-response .
* **Source**: Stanford University researchers.
* **Format**: JSON with similar structure to our educational dataset.
* **Content**: Diverse instructions covering reasoning, creativity, analysis, etc.

**Comparison with Our Educational Dataset**

<img width="476" height="326" alt="image" src="https://github.com/user-attachments/assets/eda77b16-463c-44d9-beb5-9c8d42fb1bb7" />

**Download and Load Alpaca Dataset**

You can download the original Alpaca dataset using the following code:
```
import json
import requests

def download_alpaca_dataset():
    url = "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
    response = requests.get(url)
    data = response.json()
    return data

# Load the dataset
alpaca_data = download_alpaca_dataset()
print(f"Alpaca dataset size: {len(alpaca_data)}")
```
## Loading the Pretrained GPT-2 Model
This section marks the transition from data preparation to model setup and demonstrates why instruction fine-tuning is necessary. At first, we are loading GPT-2 Medium (355M parameters) because: 
* Better capacity to learn instruction-following patterns.
* More parameters to capture nuanced relationships.
* Proven effectiveness for instruction tuning tasks.
* Reasonable size for educational purposes.
  
### The Need for Instruction Fine-Tuning
The next step is to actually perform the instruction fine-tuning process, where we'll train this pretrained model on our instruction dataset to transform it from a general text completer into an instruction-following assistant.

Before fine-tuning the model:
* ✅ Good at text completion.
* ✅ General language understanding.
* ❌ Poor instruction following.
* ❌ No structured response generation

After Fine-Tuning (Goal):
* ✅ Maintains general capabilities.
* ✅ Adds instruction-following ability.
* ✅ Structured response generation.
* ✅ Task-specific understanding.

**Hardware Requirements:** This is a crucial practical consideration for readers with different hardware setups. Model size comparison:

<img width="796" height="264" alt="image" src="https://github.com/user-attachments/assets/321326ad-3f3f-4cdc-8390-957c5b5b67df" />

**Practical Recommendations**
1. **For Readers with Limited Hardware:**
   * Start with GPT-2 Small - get the pipeline working first.
   * Use Google Colab free tier - instant access to T4 GPU.
   * Reduce batch size to 2 or 4 if memory issues persist.
   * Monitor memory usage during training.
  
2. **For Better Results:**
   * Use GPT-2 Medium if you have access to any GPU.
   * Cloud GPUs are cost-effective for this training.
   * The quality difference between small and medium is significant for instruction tuning

**Expected Results by Model Size**
1. **GPT-2 Small (124M):**
   * Basic instruction following - works for simple tasks.
   * May struggle with complex or multi-step instructions.
   * Good for learning the process.
   * Fast iteration for experiments.
  
2. **GPT-2 Medium (355M):**
   * Good instruction following - handles most tasks in the dataset.
   * Better response quality - more coherent and accurate.
   * Recommended for meaningful results.
   * Reasonable compute requirements
  
