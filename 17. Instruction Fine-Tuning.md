# Instruction Fine-Tuning
This section introduces the core concept of instruction fine-tuning. Pretrained LLMs (like base GPT-2) are excellent at text completion but poor at instruction following:
* Can complete sentences: "The weather today is..." → "sunny and warm"
* Struggle with instructions: "Fix this grammar: 'He go to school'" → Often continues incorrectly

Instruction fine-tuning (also called supervised instruction fine-tuning) transforms a pretrained LLM into one that can:
* Understand specific instructions
* Generate appropriate responses (grammar correction, text transformation, Q&A, etc.)

<img width="668" height="378" alt="image" src="https://github.com/user-attachments/assets/f4ad5893-6546-4c60-93f0-27cbe22f12e4" />

**Supervised instruction fine-tuning follows three-stage process**  

**Stage 1: Dataset Preparation**
* Step 1: Download and format the instruction dataset
* Step 2: Organize data into training and target batches

**Stage 2: Model Setup & Fine-tuning**
* Step 3: Load a pretrained LLM
* Step 4: Fine-tune on instruction data

**Stage 3: Evaluation**
* Step 5: Extract and evaluate model responses
* Step 6: Quantify performance

<img width="660" height="414" alt="image" src="https://github.com/user-attachments/assets/ca09a487-8a92-4e82-9706-861dfe13d2e5" />

## Dataset preparation
The choice of prompt style is crucial because it teaches the model the conversation structure it should follow. During instruction fine-tuning, the model learns:
* **To recognize instruction patterns:** How instructions are presented
* **When to generate responses:** The transition from input to output
* **Response formatting:** How to structure its answers appropriately

The following example show a dataset with instruction-response pairs in JSON format structure:
```
{
    'instruction': 'What is an antonym of complicated?',
    'input': '',  # Can be empty
    'output': 'An antonym of complicated is simple.'
}
```

### Alpaca Style 
Alpaca is one of the first publicly documented instruction fine-tuning approaches. It has clear structure that explicitly make the training objective very clear to the model. The alpaca is widely adopted in the open-source LLM community. Below shows the Alpaca style:
```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
[Instruction text]

### Input:
[Input text]  # Optional section

### Response:
[Expected output]
```

### Phi-3 Style
The Phi-3 style uses special tokens instead of verbose section headers. It mimics chat-based interfaces more closely and less overhead in the prompt. Below shows the Phi-3 style:
```
<|user|>
[Instruction text]
[Input text]  # If provided
<|assistant|>
[Expected output]
```
This figure is a great comparison of the two main prompt styling approaches for instruction fine-tuning.

<img width="709" height="444" alt="image" src="https://github.com/user-attachments/assets/259bb4a7-8002-4b52-b73b-c70198bc4de6" />

### Alpaca Style is Chosen for This Chapter
Both styles teach the model the same fundamental concept: recognize when it's being given an instruction and generate an appropriate response. The main difference is in how that context is presented. Different styles work better for different use cases, which is why both remain popular in the LLM ecosystem. The choice often depends on:
1. Target deployment scenario (chat vs. instruction-following)
2. Model size and capacity (larger models can handle more complex formatting)
3. Existing infrastructure (compatibility with current systems)

In this chapter, Alpaca style is used for the following reasons:

1. **Educational Value:** The explicit structure makes it easier to understand what's happening during training
2. **Debugging Friendly:** Clear section boundaries help identify issues
3. **Proven Approach:** Well-established method with documented success
4. **Transparency:** Easy to see exactly what the model is learning

### Custom Batching
This section covers the critical batching process for instruction fine-tuning, which requires special handling compared to standard classification tasks. Instruction fine-tuning has unique requirements that standard PyTorch DataLoader can't handle automatically. The 5-Step Batching Process:
1. **Step 1: Applying Prompt Template**
   * Convert raw dictionary entries into formatted text using Alpaca style
   * Creates the full training text that the model will see
2. **Step 2: Tokenization**
   * Convert formatted text into token IDs using GPT-2 tokenizer
   * Same tokenization approach used in pretraining
3. **Step 3: Adding Padding Tokens**
   * Pad sequences to the longest example in each batch (not entire dataset)
   * Uses ```<|endoftext|>``` token (**ID 50256**) for padding
   * Enables efficient GPU processing with uniform tensor shapes
4. **Step 4: Creating Target Token IDs**
   * Shift input tokens one position to the right for next-token prediction
   * Standard autoregressive training approach used in pretraining
5. **Step 5: Masking Padding Tokens**
   * Replace padding tokens in targets with -100 (PyTorch's ignore_index)
   * Prevents padding tokens from contributing to loss calculation
   * Crucial for training efficiency and accuracy

<img width="614" height="592" alt="image" src="https://github.com/user-attachments/assets/fdc4e362-5b8b-49fe-b934-c87e02765478" />

Figure below illustrates the crucial data preprocessing pipeline that transforms raw instruction data into model-ready token sequences. Let me break down these two foundational steps:

<img width="705" height="472" alt="image" src="https://github.com/user-attachments/assets/affff1bc-6f94-4a23-a25a-23420df955ea" />


### Dynamic Batch Padding
This section introduces a smart padding strategy that's crucial for efficient instruction fine-tuning. If we padded all sequences to the dataset maximum length:
* Most sequences would have excessive padding
* Wasted computation on padding tokens
* Slower training due to processing unnecessary tokens
* Memory inefficiency

**The Solution: Dynamic Batch Padding** Pad sequences only to the longest sequence in each batch, not the entire dataset. Benefits of dynamic batch padding:
1. **Computational Efficiency**
   * Less padding: Only necessary padding per batch
   * Faster training: Shorter sequences when possible
   * GPU memory optimization: Reduced memory footprint
2. **Training Quality**
   * More meaningful tokens: Higher ratio of real data vs padding
   * Better gradient updates: Loss calculated on actual content
   * Reduced overfitting: Less exposure to repetitive padding patterns
3. **Flexibility**
   * Handles variability: Accommodates diverse instruction lengths
   * Adaptive: Each batch optimized for its specific content
   * Scalable: Works with datasets of any size distribution

**Visual Example from Figure 7.8**

1. **Batch 1 (Long sequences - more padding):**
```
Input 1: [Token11, Token12, Token13, Token14, Token15]
Input 2: [Token21, Token22, <PAD>, <PAD>, <PAD>]
Input 3: [Token31, Token32, Token33, <PAD>, <PAD>]
```
2. **Batch 2 (Short sequences - less padding):**
```
Input 4: [Token41, Token42]
Input 5: [Token51, Token52, Token53]  
Input 6: [Token61, <PAD>, <PAD>]  # Padded to length 3
```

### Target sequence
Target sequence is generated by "shift right by one" operation which is the fundamental mechanism that enables next-token prediction. The core idea is train the model to predict the next token given all previous tokens. Example:

**Input Sequence (Context):**

```
Tokens:  [A,    B,    C,    D,    E]
```

**Target Sequence (What to predict):**

```
Targets: [B,    C,    D,    E,    <EOS>]
```

**Training Process:**

```
Given A → Predict B
Given A, B → Predict C
Given A, B, C → Predict D
Given A, B, C, D → Predict E
Given A, B, C, D, E → Predict <EOS>
```

The above example, highlights the critical step of target sequence preparation. the mechanism that actually teaches the model what to generate. During inference, the model generates text token-by-token **(Autoregressive Generation)**:
* Start with initial prompt
* Predict next token
* Append predicted token to context
* Repeat until <EOS> is generated

This step is essential, because without proper targets, the model doesn't know what to predict.

### Selective Masking with -100
This is a crucial optimization step that significantly improves training efficiency and quality. When we pad sequences to make them the same length within a batch, we create artificial tokens that shouldn't contribute to learning:
```
Input:   [A, B, C, <PAD>, <PAD>]
Target:  [B, C, <PAD>, <PAD>, <PAD>]  # All padding tokens affect loss!
```
This would teach the model to predict padding tokens, which is useless. The solution is using selective masking with -100. So, why -100? Because PyTorch's ```cross_entropy_loss(ignore_index=-100)``` automatically excludes these positions from loss calculation. Masking strategy is keeping the first ```<EOS>``` token but mask additional padding tokens: 

**Before Masking:**
```
Target:  [B, C, <EOS>, <PAD>, <PAD>]  # From shifting right
```

**After Masking:**
```
Target:  [B, C, <EOS>, -100, -100]    # Only meaningful tokens contribute
```

**Why Keep One EOS Token?** The model needs to learn when to stop generating. By keeping one ```<EOS>``` token:
1. **Learns stop conditions:** Understands when a response is complete.
2. **Prevents infinite generation:** Knows when to end responses.
3. **Natural conversation flow:** Mimics real dialogue endings.

**Example**

**Batch with different-length sequences:**
```
Sequence 1: [A, B, C, D, E]          → Length: 5
Sequence 2: [F, G, H]                 → Length: 3  
Sequence 3: [I, J, K, L]              → Length: 4
```

**After padding to max length 6:**
```
Inputs:   [A, B, C, D, E, <PAD>]
          [F, G, H, <PAD>, <PAD>, <PAD>]
          [I, J, K, L, <PAD>, <PAD>]

Targets:  [B, C, D, E, <EOS>, -100]      # First EOS kept
          [G, H, <EOS>, -100, -100, -100]
          [J, K, L, <EOS>, -100, -100]
```

**Impact on Training Efficiency**
1. **Without Masking:**
   * Loss calculated on all padding tokens.
   * Wasted computation (~30-50% of batches could be padding).
   * Slower convergence.
   * Poorer quality learning.
2. **With Masking:**
   * Loss only on meaningful tokens.
   * Faster training (less computation).
   * Better gradient updates.
   * Focused learning on actual content

**Difference from Classification Fine-Tuning**  
In classification, we only cared about the last token's prediction, so padding didn't affect the loss. Here, we care about every token in the sequence, so proper masking is essential.

**The PyTorch Cross-Entropy Convention**  
The ignore_index parameter tells the loss function: "When you see target value -100, skip calculating loss for that position."
```
torch.nn.functional.cross_entropy(..., ignore_index=-100)
```
The -100 convention is a great example of how thoughtful API design in deep learning frameworks enables efficient and effective model training. It's a simple mechanism that solves a very practical problem in elegant way.

### What is Instruction Masking?
Instruction masking means applying the -100 ignore index to target tokens that correspond to the instruction portion of the text, **so loss is only calculated on the response tokens**. The following example shows **without instruction masking** (Current Approach):
```
Full text: "### Instruction: Fix grammar ### Input: He go to school ### Response: He goes to school"

Input:  [###, Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :]
Target: [Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :, He, goes, to, school]
# Loss calculated on ALL tokens (instruction + response)
```

The following example show **with instruction masking** (alternative approach):
```
Input:  [###, Instruction, :, Fix, grammar, ###, Input, :, He, go, to, school, ###, Response, :]
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, He, goes, to, school]
# Loss only calculated on response tokens
```

**Reasons why Instruction Masking was suggested?**
1. **Focus on Response Generation**
   * **Primary goal:** Teach the model to generate good responses, not reconstruct instructions.
   * **Targeted learning:** Concentrates model capacity on what matters most.
   * **Clear objective:** "Only train on what you want the model to generate"
2. **Computational Efficiency**
   * **Fewer loss calculations:** Reduces unnecessary computations.
   * **Faster training:** Less backward pass through irrelevant tokens.
   * **Memory savings:** Smaller effective sequence lengths.
3. **Avoid Instruction Pattern Memorization**
   * **Prevent memorization:** Avoids learning specific instruction phrasing patterns
   * **Efficient learning:** Computational resources focused on valuable targets
   * **Better generalization:** Model learns response patterns that work across different instruction wordings
  
**The Emerging Evidence AGAINST Instruction Masking**

Despite these reasons, recent research (like Shi et al. 2024) suggests that the benefits of instruction masking may be smaller than previously thought, and in some cases, no masking can produce better results. The paper "Instruction Tuning With Loss Over Instructions" found:
1. No consistent benefits from instruction masking across various models.
2. Sometimes worse performance when masking instructions.
3. Better generalization when models learn full instruction+response patterns.
4. Holistic understanding improves with unmasked training.
5. **Suggestion:** Calculate loss over both instructions and responses for better results

**For example**  

**Input and Target Preparation - WITHOUT Instruction Masking**
```
Input:  [100, 101, 200, 201, 202, 300, 301, 400, 401, 500, 501]
Target: [101, 200, 201, 202, 300, 301, 400, 401, 500, 501, 600, 601, 602, 50256]
```
**Loss is calculated on ALL target positions:**  
```
Target: [101, 200, 201, 202, 300, 301, 400, 401, 500, 501, 600, 601, 602, 50256]
         ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑
        All these positions contribute to loss (instruction + response)
```

**Input and Target Preparation - WITH Instruction Masking**
```
Input:  [100, 101, 200, 201, 202, 300, 301, 400, 401, 500, 501]
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 600, 601, 602, 50256]
```
**Loss Calculation:**
```
Target: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 600, 601, 602, 50256]
         ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑    ↑    ↑    ↑
                Ignored                       Only these contribute to loss (response only)
```

**This chapter chooses no masking for the following reasons:**  

1. **Simpler implementation:** Fewer moving parts for educational code.
2. **Established practice:** Many successful instruction-tuned models don't use instruction masking.
3. **Research-backed:** Recent evidence suggests it might not be beneficial.
4. **Clearer learning progression:** Students can see the full training process

After loading the GPT-2 medium, we check its performance for instruction-following task. This "before" snapshot is crucial because it provides a clear baseline against which we can measure the improvements after fine-tuning, making the value of instruction tuning concrete and measurable. For example, below is an instruction that describes a task. Write a response that appropriately completes the request:
```
### Instruction:
Convert the active sentence to passive: 'The chef cooks the meal every day.'
```

**Expected response is as follow:**
```
The meal is cooked every day by the chef.
```

**Actual pretrained model response:**
```
### Response:

The chef cooks the meal every day.

### Instruction:

Convert the active sentence to passive: 'The chef cooks the
```

Analysis of pretrained model behavior show the model for the following reasons created a wrong response:
1. **Text completion mindset:** The model treats it as text continuation, not instruction execution.
2. **Pattern repetition:** Simply repeats parts of the instruction and input.
3. **No task understanding:** Doesn't comprehend the "convert to passive" request.
4. **Format confusion:** Starts new instruction sections instead of completing the response

**Why This Happens:**
* **Pretrained on general text:** Learned to complete text patterns, not follow instructions.
* **No instruction-response training:** Never saw structured instruction datasets during pretraining.
* **Autoregressive bias:** Predicts what comes next based on training data patterns.
* **Lack of specialization:** General language model, not specialized for instructions.

### The Original Alpaca Dataset
This is an excellent dataset for readers who want to scale up their instruction fine-tuning with a real-world dataset. The specifications of this Alpaca dataset is as follow:
* **Size**: 52,002 instruction-response .
* **Source**: Stanford University researchers.
* **Format**: JSON with similar structure to our educational dataset.
* **Content**: Diverse instructions covering reasoning, creativity, analysis, etc.

**Comparison with Our Educational Dataset**

<img width="476" height="326" alt="image" src="https://github.com/user-attachments/assets/eda77b16-463c-44d9-beb5-9c8d42fb1bb7" />

**Download and Load Alpaca Dataset**

You can download the original Alpaca dataset using the following code:
```
import json
import requests

def download_alpaca_dataset():
    url = "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
    response = requests.get(url)
    data = response.json()
    return data

# Load the dataset
alpaca_data = download_alpaca_dataset()
print(f"Alpaca dataset size: {len(alpaca_data)}")
```
## Loading the Pretrained GPT-2 Model
This section marks the transition from data preparation to model setup and demonstrates why instruction fine-tuning is necessary. At first, we are loading GPT-2 Medium (355M parameters) because: 
* Better capacity to learn instruction-following patterns.
* More parameters to capture nuanced relationships.
* Proven effectiveness for instruction tuning tasks.
* Reasonable size for educational purposes.
  
### The Need for Instruction Fine-Tuning
The next step is to actually perform the instruction fine-tuning process, where we'll train this pretrained model on our instruction dataset to transform it from a general text completer into an instruction-following assistant.

Before fine-tuning the model:
* ✅ Good at text completion.
* ✅ General language understanding.
* ❌ Poor instruction following.
* ❌ No structured response generation

After Fine-Tuning (Goal):
* ✅ Maintains general capabilities.
* ✅ Adds instruction-following ability.
* ✅ Structured response generation.
* ✅ Task-specific understanding.

**Hardware Requirements:** This is a crucial practical consideration for readers with different hardware setups. Model size comparison:

<img width="796" height="264" alt="image" src="https://github.com/user-attachments/assets/321326ad-3f3f-4cdc-8390-957c5b5b67df" />

**Practical Recommendations**
1. **For Readers with Limited Hardware:**
   * Start with GPT-2 Small - get the pipeline working first.
   * Use Google Colab free tier - instant access to T4 GPU.
   * Reduce batch size to 2 or 4 if memory issues persist.
   * Monitor memory usage during training.
  
2. **For Better Results:**
   * Use GPT-2 Medium if you have access to any GPU.
   * Cloud GPUs are cost-effective for this training.
   * The quality difference between small and medium is significant for instruction tuning

**Expected Results by Model Size**
1. **GPT-2 Small (124M):**
   * Basic instruction following - works for simple tasks.
   * May struggle with complex or multi-step instructions.
   * Good for learning the process.
   * Fast iteration for experiments.
  
2. **GPT-2 Medium (355M):**
   * Good instruction following - handles most tasks in the dataset.
   * Better response quality - more coherent and accurate.
   * Recommended for meaningful results.
   * Reasonable compute requirements
  
## LLM Evaluation
LLM evaluation fundamentally different from traditional ML evaluation and requires multiple approaches. The three main evaluation paradigms for instruction-tuned LLMs:
1. **Multiple-Choice & Knowledge Benchmarks**
These benchmarks test a model's factual knowledge, reasoning capabilities, and problem-solving skills through structured question-answer formats.

   * **MMLU (Massive Multitask Language Understanding)**
     - Scope: 57 subjects across STEM, humanities, social sciences.
     - Format: 4-choice multiple choice questions.
     - Size: ~16,000 questions
```
# Example MMLU question
question = "What is the capital of France?"
choices = ["London", "Berlin", "Paris", "Madrid"]
correct_answer = 2  # Paris
```

   * **HellaSwag**
     - Purpose: Commonsense reasoning about physical situations
     - Format: Sentence completion with 4 options
```
A woman is outside with a bucket. She...
A) uses the bucket to play in the sand
B) fills the bucket with water and uses it to clean the car  ✓
C) plays with the bucket and it's windy
D) gets in the bucket and it's raining
```
   * **TruthfulQA**
     - Focus: Truthfulness and avoidance of false information
     - Format: Questions where models might reproduce common misconceptions
```
Question: "Can you get a cold from being cold?"
Common misconception: "Yes, that's why it's called a cold"
Truthful answer: "No, colds are caused by viruses"
```
   * **GSM8K (Grade School Math 8K)**
     - Focus: Multi-step mathematical reasoning
     - Format: Grade school math word problems
```
"John has 5 apples. He gives 2 to Mary and buys 3 more. How many apples does he have now?"
Solution: 5 - 2 + 3 = 6
```

2. **Human Preference Evaluation**
Human preference evaluation involves real humans judging and comparing model outputs to assess quality, helpfulness, and alignment with human values.

   * **LMSYS Chatbot Arena**
     - Method: Blind A/B testing where humans compare two anonymous model responses.
     - Output: Elo ratings, win rates, and rankings.
```
Human User: "Explain quantum computing like I'm 5"
     ↓
Model A Response: [hidden identity]
Model B Response: [hidden identity] 
     ↓
Human chooses: A ✓ or B ✓ or Tie
     ↓
Elo ratings updated
```
   * **Direct Human Rating**
     - Method: Humans rate individual responses on Likert scales (Rate responses on quality dimensions: 1-5 scale).
     - Dimensions: Helpfulness, accuracy, coherence, safety, etc.

3. **Automated Conversational Benchmarks**
These use powerful LLMs as judges to automatically evaluate model responses, providing scalable, consistent assessment of conversational quality.

   * **AlpacaEval**
     - Method: Uses GPT-4 as judge to compare responses against a baseline
     - Metric: Win rate against text-davinci-003
```
Instruction → Candidate Model → Response A
             ↓
             Baseline Model → Response B
             ↓
             GPT-4 Judge → "A is better" / "B is better" / "Tie"
```

  * **Vicuna-Bench**
    - Judge: GPT-4 assesses response quality.
    - Output: Scores and qualitative feedback

Automated conversational benchmarks have become the standard for rapid iteration in LLM development because they provide reasonably accurate quality assessments at a fraction of the cost and time of human evaluation, while being much better at measuring conversational quality than multiple-choice knowledge tests.

### Automated Evaluation
This section explains the practical implementation choice for automated evaluation. We're using a pre-trained, capable LLM (Llama 3) as an "AI judge" to evaluate the responses from our fine-tuned GPT-2 model. **Ollama** is an open-source application that makes it easy to run large language models (LLMs) locally on your own computer. Ollama lets you download and run LLMs on your laptop or desktop without needing:
* Internet connection
* API keys
* Cloud services
* Monthly fees

Ollama is like having a "AI assistant in a box" that you can run completely offline. It democratizes access to powerful AI models by making them as easy to use as any other desktop application. 

**Key Features of Ollama are:**
1. ✅ **Command Line:** Simple terminal commands
2. ✅ **Web API:** REST API for programmatic access
3. ✅ **Written in C++:** Maximum performance
4. ✅ **Quantization:** Model compressed from ~16GB to 4.7GB with minimal quality loss
5. ✅ **Hardware optimization:** Uses CPU efficiently, supports GPU acceleration
6. ✅ **Optimized**: Runs efficiently on consumer hardware
7. ❌ **Training:** Creating new models (what Ollama doesn't do)

The Evaluation Architecture with Ollama can be summarized as:
```
Our Fine-tuned GPT-2 (355M params)
         ↓
   Generates responses to test instructions
         ↓
   Responses saved for evaluation
         ↓
Llama via Ollama
         ↓
   Scores each response (0-100)
         ↓
   Average score = Model performance metric
```

**Comparison to Alternatives**

<img width="508" height="226" alt="image" src="https://github.com/user-attachments/assets/3b1b954c-8bb4-4274-a619-751f6d4766ac" />

**Installation & Setup**
1. Download from ollama.com
2. Install like any normal application
3. Open terminal and run: ollama pull llama3
4. Start chatting: ollama run llama3

**Model options and hardware requirements**

Any of the following models will work for evaluating our fine-tuned GPT-2 model. This flexibility ensures that every one can complete the practical evaluation portion of the chapter, which is the most important learning objective.

<img width="589" height="211" alt="image" src="https://github.com/user-attachments/assets/96798ca9-a409-4f21-9624-0da8f750fad1" />

**Run Commands**
```
# Run the standard 8B model
ollama run llama3

# Run the smaller Phi-3 model  
ollama run phi3

# Run the large 70B model
ollama run llama3:70b
```

**Verification:**
```
# Check if Ollama is working
ollama --version

# List downloaded models
ollama list

# Test with a simple question
ollama run llama3 "What is 2+2?"
```

**Example Interaction:**
```
$ ollama run llama3
>>> What do llamas eat?
Llamas are ruminant animals, which means they have a four-chambered
stomach and eat plants that are high in fiber. In the wild, 
llamas typically feed on:

1. Grasses: They love to graze on various types of grasses, including tall
grasses, wheat, oats, and barley.
```
We use Ollama to run Llama 3 as an "AI judge" to evaluate our fine-tuned model's responses - a practical, free, and accessible solution that works for all students regardless of their budget or internet connectivity.
