{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ac24cc-d894-49f9-ab85-f295e7c68bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose we have the following token embedding vector.\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x3)\n",
    "   [0.22, 0.58, 0.33], # with     (x4)\n",
    "   [0.77, 0.25, 0.10], # one      (x5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e614b81-9d1a-4fcb-8464-4cc9104e522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code implements a self-attention mechanism using PyTorch. Here's a breakdown:\n",
    "d_in = inputs.shape[1]  # d_in: Input dimension (e.g., 3 for our token embeddings)\n",
    "d_out = 2               # d_out: Output dimension (e.g., 2 for smaller context vectors)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class SelfAttention(nn.Module): # Defines a neural network module for self-attention.\n",
    "    \n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # The \"bias\" parameter in the linear layers controls whether to include a bias term in the Query, Key, and Value projections.\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)  # W_query: Projects input to query vectors (what to look for)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)  # W_key: Projects input to key vectors (what to offer)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)  # W_value: Projects input to value vectors (actual content)\n",
    "\n",
    "    \n",
    "    # Transforms input x (token embeddings) into queries, keys, and values.\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T  # Computes attention scores via dot products between all query-key pairs.\n",
    "\n",
    "        # Applies softmax to convert scores to probabilities (weights sum to 1 per row).\n",
    "        attn_weights = torch.softmax(\n",
    "            # Uses scaling (/ keys.shape[-1]**0.5) for stability.\n",
    "            # Scales scores by √d_k (here, d_k = d_out = 2) to prevent gradient issues.\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1 \n",
    "        )\n",
    "\n",
    "        # Computes context vectors as weighted sums of value vectors (using attention weights).\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84929d37-1f11-41b9-a525-3d3eb7b84092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# use the SelfAttention_v2\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3099547e-aff7-41dd-aa8f-7fdeaf603593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2034, 0.1635, 0.1642, 0.1501, 0.1741, 0.1446],\n",
      "        [0.2212, 0.1650, 0.1654, 0.1425, 0.1658, 0.1401],\n",
      "        [0.2204, 0.1651, 0.1655, 0.1429, 0.1657, 0.1405],\n",
      "        [0.1958, 0.1665, 0.1667, 0.1531, 0.1657, 0.1522],\n",
      "        [0.1902, 0.1668, 0.1670, 0.1556, 0.1654, 0.1551],\n",
      "        [0.2054, 0.1659, 0.1662, 0.1490, 0.1662, 0.1472]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check the values of queries, keys, attn_scores, and attn_weights\n",
    "queries = sa_v2.W_query(inputs)     #1\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5b9caa-eaed-429e-b7e7-5683fda6fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Hiding future words with causal attention\n",
    "\n",
    "# attn_scores is a 6*6 matrix (for our 6-token sequence)\n",
    "# context_length = 6 (number of tokens)\n",
    "context_length = attn_scores.shape[0] \n",
    "\n",
    "# torch.ones(6, 6) creates a 6×6 matrix filled with 1s\n",
    "#torch.triu(..., diagonal=1) extracts the upper triangular\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "# mask.bool() converts the mask to boolean (True/False)\n",
    "# masked_fill replaces True positions with -torch.inf (negative infinity)\n",
    "# Effect: Future token scores become -inf, which becomes 0 after softmax.\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec95056-526c-48aa-b0ce-1d7bd606f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5728, 0.4272, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4000, 0.2996, 0.3004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2870, 0.2441, 0.2444, 0.2245, 0.0000, 0.0000],\n",
      "        [0.2251, 0.1974, 0.1976, 0.1842, 0.1957, 0.0000],\n",
      "        [0.2054, 0.1659, 0.1662, 0.1490, 0.1662, 0.1472]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now all we need to do is apply the softmax function to these masked results, and we are done:\n",
    "\n",
    "\"\"\"\n",
    " applies the softmax function to the masked attention scores to convert them into valid attention weights.\n",
    " dim=1: Apply softmax across each row (for each token's attention distribution)\n",
    " exp(-inf) = 0\n",
    "\"\"\"\n",
    "attn_weights = torch.softmax(masked , dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d067ade6-35b2-4540-9c1b-b4ef30244bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8544, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6008, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5739, 0.4882, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4501, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3318, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Masking additional attention weights with dropout\n",
    "\"\"\"\n",
    "    Dropout is a regularization technique.\n",
    "    Randomly \"turns off\" (sets to zero) a fraction of neurons or weights during training to prevent overfitting.\n",
    "    0.5: Dropout rate (50% of values will be set to zero during training)\n",
    "    The remaining 50% are scaled up by 2× (1/0.5 = 2) to maintain the overall magnitude\n",
    "\"\"\"\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) \n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f8ec37-32df-435d-b5a6-66f40f77ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a compact causal attention class\n",
    "\n",
    "\"\"\"\n",
    "    d_in: Input dimension (e.g., embedding size)\n",
    "    d_out: Output dimension for queries/keys/values\n",
    "    context_length: Maximum sequence length the model can handle\n",
    "    dropout: Dropout rate (e.g., 0.5 for 50%)\n",
    "    qkv_bias: Whether to include bias in Q/K/V projections\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "\n",
    "        \"\"\"\n",
    "         The \"bias\" parameter in the linear layers controls whether to include a bias term in the Query, Key, and Value projections.\n",
    "          W_query: Projects input to query vectors (what to look for)\n",
    "          W_key: Projects input to key vectors (what to offer)\n",
    "          W_value: Projects input to value vectors (actual content)\n",
    "        \"\"\"\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)    # Learnable projections for Query\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)    # Learnable projections for Key\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)    # Learnable projections for Value\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)   # Dropout layer for regularization\n",
    "\n",
    "        \"\"\"\n",
    "          torch.ones(6, 6) creates a 6×6 matrix filled with 1s\n",
    "           torch.triu(..., diagonal=1) extracts the upper triangular\n",
    "        \"\"\"\n",
    "        self.register_buffer(\n",
    "           'mask',\n",
    "           torch.triu(torch.ones(context_length, context_length),\n",
    "           diagonal=1)\n",
    "        )  \n",
    "        \n",
    "    # Transforms input x (token embeddings) into queries, keys, and values.\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape        # Extract batch size, actual sequence length, and input dim\n",
    "        keys = self.W_key(x)                 # Project input to Key. shape [batch_size, num_tokens, d_out]\n",
    "        queries = self.W_query(x)            # Project input to Query. shape [batch_size, num_tokens, d_out]\n",
    "        values = self.W_value(x)             # Project input to Value. shape [batch_size, num_tokens, d_out]\n",
    "\n",
    "        \"\"\"\n",
    "         Computes attention scores via dot products between all query-key pairs.\n",
    "         Result attn_scores: [b, T, T] (batch_size, num_tokens, num_tokens)\n",
    "        \"\"\"\n",
    "        attn_scores = queries @ keys.transpose(1, 2) \n",
    "\n",
    "        \"\"\"\n",
    "            mask.bool() converts the mask to boolean (True/False)\n",
    "            masked_fill replaces True positions with -torch.inf (negative infinity)\n",
    "            Effect: Future token scores become -inf, which becomes 0 after softmax.\n",
    "        \"\"\"\n",
    "        attn_scores.masked_fill_(                    \n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "\n",
    "        \"\"\"\n",
    "             Applies the softmax function to the masked attention scores to convert them into valid attention weights.\n",
    "             dim=1: Apply softmax across each row (for each token's attention distribution)\n",
    "             exp(-inf) = 0\n",
    "        \"\"\"\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=1\n",
    "        )\n",
    "       \n",
    "        \"\"\"\n",
    "          Dropout is a regularization technique.\n",
    "          Randomly \"turns off\" (sets to zero) a fraction of neurons or weights during training to prevent overfitting.\n",
    "          0.5: Dropout rate (50% of values will be set to zero during training)\n",
    "          The remaining 50% are scaled up by 2× (1/0.5 = 2) to maintain the overall magnitude\n",
    "        \"\"\"\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        # Return context-enriched representations\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c173402-6bf1-490d-b2f0-42e5e4f039c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
    "   For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "   inputs: A single sequence tensor of shape [6, 3] (6 tokens, 3-dimensional embeddings)\n",
    "   torch.stack((inputs, inputs), dim=0):\n",
    "      Takes two copies of inputs\n",
    "      Stacks them along a new dimension at position 0 (batch dimension)\n",
    "      Creates shape [2, 6, 3], batch.shape: torch.Size([2, 6, 3])\n",
    "      [2, 6, 3] = [batch_size, num_tokens, embedding_dim]\n",
    "         2: Number of sequences in batch\n",
    "         6: Tokens per sequence\n",
    "         3: Dimensions per token embedding\n",
    "\"\"\"\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)  \n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "\"\"\"\n",
    "   This code initializes and uses the causal attention mechanism on a batch of input sequences:\n",
    "   Creates a causal attention instance with:\n",
    "     d_in: Input dimension (3 - embedding size of each token)\n",
    "     d_out: Output dimension (2 - size of query/key/value vectors)\n",
    "     context_length: Maximum sequence length (6 - tokens per sequence)\n",
    "     0.0: Dropout rate (0% dropout = no regularization)\n",
    "\"\"\"\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "\"\"\"\n",
    "    Processes the input batch through the attention mechanism:\n",
    "    batch: Shape [2, 6, 3] (2 sequences, 6 tokens each, 3D embeddings)\n",
    "    context_vecs: Output shape [2, 6, 2] (2 sequences, 6 tokens, 2D context vectors)\n",
    "\"\"\"\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print('context_vecs.shape:', context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa373725-6e47-4f43-9115-605da2c989e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing multi-head attention with weight splits\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensures the output dimension can be split evenly among heads\n",
    "        # Example: If d_out=8 and num_heads=4, each head gets 2 dimensions\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads    \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)# Single large projection matrices (more efficient than separate ones per head)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   # Optional layer to mix information from different heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)         # [b, num_tokens, d_out]\n",
    "        queries = self.W_query(x)    # [b, num_tokens, d_out]\n",
    "        values = self.W_value(x)     # [b, num_tokens, d_out]\n",
    "\n",
    "        \"\"\"\n",
    "            Reshape for Multiple Heads\n",
    "            Reshapes [b, T, d_out] → [b, T, h, d_h] where d_out = h * d_h\n",
    "            Example: [2, 6, 8] → [2, 6, 4, 2] (4 heads, 2 dims each)\n",
    "        \"\"\"\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim                    \n",
    "        )                                                                   \n",
    "\n",
    "        \"\"\"\n",
    "            Transpose for Batch Computation\n",
    "            Rearranges to [batch, heads, tokens, dims_per_head]\n",
    "            Allows parallel computation across heads\n",
    "        \"\"\"\n",
    "        keys = keys.transpose(1, 2)          # [b, h, T, d_h]\n",
    "        queries = queries.transpose(1, 2)    # [b, h, T, d_h]\n",
    "        values = values.transpose(1, 2)      # [b, h, T, d_h]\n",
    "\n",
    "        \"\"\"\n",
    "            Compute Attention Scores\n",
    "            Batched matrix multiplication across all heads\n",
    "            Computes all attention scores in parallel\n",
    "        \"\"\"\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "\n",
    "        \"\"\"\n",
    "            Apply Causal Mask\n",
    "            Uses pre-computed triangular mask\n",
    "            Blocks future tokens for autoregressive generation\n",
    "        \"\"\"\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        \n",
    "        # Softmax: Standard scaled softmax attention\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        \"\"\"\n",
    "            Apply Attention to Values\n",
    "            Weighted sum of values\n",
    "            ranspose back to [batch, tokens, heads, dims]\n",
    "        \"\"\"\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)   # [b, T, h, d_h]\n",
    "\n",
    "        \"\"\"\n",
    "            Combine Heads\n",
    "            Flatten heads: [b, T, h, d_h] → [b, T, h*d_h] = [b, T, d_out]\n",
    "            Example: [2, 6, 4, 2] → [2, 6, 8]\n",
    "        \"\"\"\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "            Output Projection\n",
    "            Optional linear transformation\n",
    "            Helps mix information across heads\n",
    "        \"\"\"\n",
    "        context_vec = self.out_proj(context_vec)    #11\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7faa65c-0285-4412-a5fb-900fa524e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "#The MultiHeadAttention class can be used similar to the SelfAttention and CausalAttention classes we implemented earlier:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)  \n",
    "d_in = inputs.shape[1]  # d_in: Input dimension (e.g., 3 for our token embeddings)\n",
    "d_out = 2               # d_out: Output dimension (e.g., 2 for smaller context vectors)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print('context_vecs.shape:', context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
