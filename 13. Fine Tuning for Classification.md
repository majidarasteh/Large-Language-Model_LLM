<img width="704" height="390" alt="image" src="https://github.com/user-attachments/assets/e2fdd6c2-b919-47e6-95c2-ae566592c668" /># Fine Tuning for Classification
In this chapter, we transition from building and pretraining a general-purpose language model to specializing it for a concrete real-world task: **text classification**. We focus on fine-tuning a pretrained LLM to distinguish between “spam” and “not spam” messages—a classic binary classification problem. You will learn the key differences between instruction fine-tuning, which enables models to perform diverse tasks based on natural language prompts, and classification fine-tuning, which tailors a model to predict specific, predefined labels. Through hands-on examples, we cover the entire process: preparing a labeled dataset, modifying the model’s architecture, implementing the training loop, evaluating performance, and deploying the fine-tuned classifier. By the end of this chapter, you will have the skills to adapt LLMs for specialized classification tasks across various domains.

## Two Primary Fine-Tuning Paradigms
1. **Instruction Fine-Tuning**: Creates a generalist model that follows instructions. For example, the model can be asked "Is this spam?" or "Translate this to German." This flexibility is the key takeaway.
2. **Classification Fine-Tuning**: Creates a specialist model that maps inputs to a fixed set of predefined labels (spam/ham). Its strength is its narrow focus and simplicity for the task at hand.

The concept of a **specialist vs. a generalist** is a powerful and intuitive way to frame the difference. It helps readers understand the trade-off:
1. **Specialist (Classification-Tuned)**: Easier and cheaper to build, highly accurate at its specific job, but inflexible. It's a master of one trade.
2. **Generalist (Instruction-Tuned)**: More complex and expensive to build, versatile across many tasks, but might not be the absolute best at any single one compared to a specialist. It's a jack-of-all-trades.

The following figure illustrates two distinct scenarios for instruction fine-tuning and classification fine-tuning. This contrast demonstrates the core strength of instruction fine-tuning: it produces a generalist model capable of understanding and executing a wide range of tasks expressed through natural language prompts, rather than being limited to a single predefined function like a spam classifier.

<img width="911" height="405" alt="image" src="https://github.com/user-attachments/assets/11b0770f-6420-426e-a41d-30b816a8b536" />

### How this Sets Up the Rest of the Chapter

The chapter will now logically progress through the steps of creating a specialist model:
1. **Data Preparation**: How to get and prepare a dataset of spam/ham messages.
2. **Model Modification**: How to surgically alter the general-purpose GPT architecture (changing the output layer) to make it a specialist classifier.
3. **The Fine-Tuning Process**: How to train this modified model on the new data.
4. **Evaluation**: How to measure the performance of our new specialist.
5. **Usage**: How to use the finished model to classify new messages.

## Data Preparation

The first step in our classification task is to prepare the dataset:

* **Dataset**: We begin by downloading a real-world collection of SMS messages labeled as "spam" or "ham" (non-spam).
* **Imbalance Handling**: A common challenge in machine learning is dealing with imbalanced data, and this dataset is no exception, containing significantly more "ham" messages than "spam."
* **Balancing**: To ensure our model learns to identify both classes effectively and isn't biased toward the majority class, we rectify this imbalance by **undersampling** the "ham" instances to create a balanced dataset with an equal number of spam and non-spam examples.
* **Train/Validation/Test Split**: Finally, we split this refined dataset into standard **training (70%), validation (10%), and test (20%)** sets. This partitioning allows us to train the model, tune its parameters, and ultimately evaluate its performance on completely unseen data, providing a robust foundation for the fine-tuning process.

### Creating data loaders
To efficiently feed the text data into the model during training, we create PyTorch DataLoaders. Since messages have different lengths, we must standardize them for batching. Here the core problem is the variable-length input:
1. **Past Approach**: Previously, text was split into uniform chunks (e.g., using a sliding window), so every input was the same size, making them easy to batch together.
2. **New Challenge**: Individual text messages (like SMS) are all different lengths. A neural network requires inputs within a batch to be the same dimensions.

To solve the variable-length problem, the text must be standardized. The two common methods are:

1. **Truncation**: Chopping off the end of longer messages to make all messages as short as the shortest one.
   * **Pro**: Fast and uses less memory.
   * **Con**: You lose information! If the important part of a message is at the end, it gets thrown away, which can hurt the model's performance.
     
2. **Padding**: Adding a special, meaningless padding token to the end of shorter messages to make them all as long as the longest message.
   * **Pro**: Preserves all the original information in every message.
   * **Con**: Uses more memory and computation because the model has to process all those extra padding tokens.

We chose padding to avoid losing any important information from the messages. The technical implementation is as follow:
1. The model doesn't understand words; it understands numbers (token IDs).
2. Therefore, we don't pad with the word ```<|endoftext|>```. Instead, we pad with its corresponding **token ID: 50256**.
3. This is done after the text has been converted into a list of numbers (tokenized). So, a short message like ```["Hi", "there"]``` (IDs ```[123, 456]```) might become ```[123, 456, 50256, 50256, 50256]``` to match the length of a longer message.

This process is about making all text messages the same length by adding a special **"dummy" number (50256)** to the end of shorter messages. This allows the computer to efficiently process many messages at once in a batch without losing any of the original text content. For example the following figure shows padding:

<img width="911" height="459" alt="image" src="https://github.com/user-attachments/assets/370d2a6c-d16b-46ff-aca5-2473e92f8fd1" />


In the text classification, there is a fundamental shift in the model's objective and its original design. There is a shift from next token prediction to classification.  

* **Original Purpose (Pretraining):** The model was trained to predict the next token in a sequence.
* **New Purpose (Fine-tuning for Classification):** he model is now trained to predict a single, overall class label for an entire input sequence.

In the text classification, as an example, the structure of the data that will be fed into the model during training could be as follow:

* **input_batch**: A tensor with dimensions [8, 120].
  - **8** is the batch size (8 independent text messages processed together).
  - **120** is the sequence length. Each message has been padded/truncated to be exactly 120 tokens long. This is the uniform length required for batching.
    
* **target_batch**: A tensor with dimensions [8].
  - This is not a sequence. It's a simple list of 8 numbers.
  - Each number is the correct class label (0 or 1) for the corresponding message in the input_batch.

<img width="640" height="585" alt="image" src="https://github.com/user-attachments/assets/1d157efc-d277-4a6e-961b-ede0e9f1849d" />

### Loading a Pre-Trained Base Model 
This section describes the crucial step of loading a pre-trained base model before adapting it for a new task. The objective is not to train a model from scratch but to leverage the general language understanding a model has already gained during its pre-training on a massive text corpus. This is more efficient and effective than training a classifier on spam messages from scratch. The process of loading and verification is as follow:

1. **Initialize with Pre-trained Weights**: First we load the parameters (weights) from a pre-existing model (like GPT-2) into a built GPT model architecture. This gives you a model that is already a powerful text generator and language understander.
2. **Sanity Check**: It's crucial to verify that the weights were loaded correctly. The text does this by using a text generation utility on a simple prompt. If the model produces coherent, sensible text (e.g., completing "Every effort moves you" with "forward."), it confirms the model is functioning as a proper language model.
3. **Test Zero-Shot Ability (Baseline)**: Before any fine-tuning, the text checks if the model can already perform the desired task ("Is this spam?") just based on its pre-training. This is called zero-shot performance.
   * **The Result:** The model fails. It doesn't answer the question; it just continues generating text in the same style. For example, given the prompt "Is the following text 'spam'? ...", it might just repeat the question or continue the sentence instead of answering "yes" or "no". This "failure" is a critical teaching point. It demonstrates a key concept:
     - A model that is only pre-trained (on a next-word prediction task) is a **general-purpose text completer**. It has no inherent ability to follow instructions or perform specific classification tasks unless it is specifically trained to do so.
     - This failed test establishes the baseline and provides the motivation for the next step: **we must modify and fine-tune** this general model to make it a specialized spam classifier.

In summary, this section is about taking a powerful, pre-trained generator and confirming that, in its current state, it is useless for our specific task. This sets the stage for the necessary architectural changes and training process (fine-tuning) to turn this generalist into a specialist.


### Architectural Modification 
This section describes the key architectural modification required to turn a generative language model into a classifier. The core modification is swapping the output layer (the "head") to change its purpose.
1. **Original Head (for Generation):**
   * **Function**: Predicts the next word in a sequence.
   * **Architecture**: A linear layer that maps the model's internal representation (e.g., 768-dimensional vectors) to a score for every possible token in its massive vocabulary (50,257 options).
   * **Output**: A vector of size 50,257.
     
2. **New Classification Head (for Spam Detection):**
   * **Function**: Predicts a single class label for the entire input sequence.
   * **Architecture**: A new linear layer that maps the same internal representation (768-dimension) to a score for each of the two possible classes ("spam" and "not spam").
   * **Output**: A vector of size 2.

**Why Two Output Nodes? (The Important Detail)** even for a binary (yes/no) task, we use two output nodes instead of one.  

1. **Technical Reason (The "How"):** Using two nodes allows them to use the standard cross-entropy loss function directly, which is the default for classification tasks in frameworks like PyTorch. This loss function is designed to compare a distribution over multiple classes (from the model) against a true label.
2. **Practical Reason (The "Why"):** This approach is more generalizable. The same code and model architecture can be instantly reused for a classification task with 3, 10, or 100 classes just by changing the number of output nodes. Using a single output node would require a different loss function (like Binary Cross-Entropy) and changes to the training code.

**The Resulting Data Flow**:  After this change, for a given input text, the model no longer produces a list of next-token probabilities. Instead, it produces two numbers (logits):
* One score for class 0 ("not spam")
* One score for class 1 ("spam")

The class with the higher score is the model's prediction. As the following figure, this simple swap is the primary architectural step needed to repurpose a powerful generative model for a specific discriminative task.

<img width="599" height="760" alt="image" src="https://github.com/user-attachments/assets/53c2192a-2b20-4386-b737-3bc68b1aa33b" />

## Fine-tuning selected layers

This section explains a crucial strategy in transfer learning: which parts of a pre-trained model to update during fine-tuning. Instead of updating all millions (or billions) of parameters of the pre-trained model, you can often get excellent results by only training a small subset of them. This is based on the understanding that different layers in a neural network capture different types of information.
1. **Lower Layers (Frozen):** The early layers of the model learn very general, fundamental features of language—things like basic grammar, syntax, and common word meanings. These features are useful for almost any NLP task (translation, summarization, classification), so they don't need to change. We "freeze" these layers by setting ```requires_grad = False```, preventing their weights from being updated during training.
2. **Higher Layers (Unfrozen)**: The layers closer to the output learn more complex, task-specific patterns and nuances. To adapt the general-purpose model to our specific task (spam detection), we unfreeze these layers (```requires_grad = True```) so they can be fine-tuned.

**The process is as follow**
1. **Freeze Everything**: The first step is to make all parameters in the pre-trained model non-trainable (```requires_grad = False```). This locks in all the existing knowledge.
2. **Replace the Head**: Swap the final output layer (the "head") from a 50,257-unit layer (for word prediction) to a new, randomly initialized layer with 2 units (for spam classification). By default, this new layer is trainable (```requires_grad = True```).
3. **Unfreeze Selectively**: Based on experimentation, they find that performance improves by also unfreezing and fine-tuning more than just the new output layer. Specifically, they unfreeze:
   * **The last transformer block**: This allows the model to adjust the highest-level, most task-specific representations it creates before making a decision.
   * **The final LayerNorm layer**: This normalization layer right before the output head is crucial for stabilizing the inputs to the new classification layer.

**For three reasons this approach is effective:**
1. **Computational Efficiency**: Training only the last few layers and the new head is much faster and requires less memory than training the entire model. This makes experimentation easier and cheaper.
2. **Prevents Catastrophic Forgetting**: By freezing the early layers, the model retains all its general language understanding. If you fine-tuned everything, you might "forget" its general knowledge and become worse at tasks other than spam detection (a phenomenon known as catastrophic forgetting).
3. **Faster Convergence**: The model already has good features. You only need to slightly adjust the high-level concepts and learn a simple mapping from those features to your new classes. This allows the model to learn the new task very quickly.

This process is like taking a highly educated expert (the pre-trained model), giving them a new, specific job description (the new classification head), and allowing them to slightly adjust their recent thought processes (the last layers) to excel at this new job, without forcing them to re-learn everything they know from the ground up.

For example, Let's break down what happens with the input text ```'Do you have time'``` in the modified model.
1. **Tokenization & Input Format**
   * The text is converted into a sequence of token IDs using the GPT-2 tokenizer. The exact IDs depend on the specific tokenizer's vocabulary.
   * Input Text: 'Every effort moves you'
   * Tokenized Sequence (Example IDs): Let's assume it tokenizes to 4 tokens with the following example IDs:
     - Inputs: tensor(```[[5211,  345,  423,  640]]```)
     - Input Tensor Shape: ```torch.Size([1, 4])``` (1 batch, 4 tokens)

2. **Model Output Format**
   * After passing through the model (with its new classification head), the output is a tensor of logits (raw prediction scores) for each token position.
   * **Before (Generation)**: The model's output had a shape of ```[batch_size, num_tokens, vocab_size]``` (e.g., ```[1, 4, 50257]```). For each of the 4 input token positions, it predicted a distribution over 50,257 possible next tokens.
   * **After (Classification)**: The new output layer changed the shape to ```[batch_size, num_tokens, num_classes]``` (e.g., ```[1, 4, 2]```). For each token position, it now outputs two numbers (logits) representing scores for "ham" (class 0) and "spam" (class 1).
     - **1**: Batch size
     - **4**: Number of input tokens (sequence length)
     - **2**: Number of classes (0: "not spam", 1: "spam")

3. **The Classification Decision**
   * As explained, we ignore the outputs for all but the last token because it contains the cumulative information from the entire sequence.
   * How GPT processes tokens:
     - The first token can only attend to itself. Its representation is based on a very limited context.
     - The second token can attend to the first token and itself.
     - The third token can attend to the first, second, and itself.
     - The last (fourth) token can attend to all previous tokens and itself. It has the most complete and comprehensive understanding of the entire input sequence.
   * Logits for the last token: [-3.5983, 3.9902]
     - Score for Class 0 ("not spam"): -3.5983
     - Score for Class 1 ("spam"): 3.9902
   * Prediction: The model predicts the class with the highest score. Here, Class 1 (spam) has the higher score (3.9902 > -3.5983).
  
4. **The Intuition and Implementation**
   *  The most informed "decision" or "summary" of the sequence is contained in the final token's representation.
   *  The code outputs ```[:, -1, :]``` implements this:
     - **":"**  Select all items in the batch (in this case, just one).
     - **"-1"**  Select the last token's position in the sequence.
     - **":"**  Select all class scores for that token.

 <img width="516" height="744" alt="image" src="https://github.com/user-attachments/assets/1a6e5ed8-1c9b-49da-bce8-0b4177781ceb" />
 
By using the last token's output, we are leveraging the model's inherent architecture. The last token's representation is a function of every token that came before it, making it the natural and most informed choice for a classification decision about the entire sequence. This is why we ignore the outputs for all previous token positions during fine-tuning.

## Measure Model Performance Through Loss and Accuracy
This section defining how to measure model performance through loss and accuracy. Here's a concise explanation of the process:
1. **From Logits to Prediction**
   * **Goal**: Convert the model's raw output (logits) for the last token into a class prediction (0 or 1).
   * **Softmax** While softmax converts logits to probabilities (e.g., ```[0.0004, 0.9996]```).
   * **Method**: Use ```argmax``` to select the class with the highest logit value.
       
     <img width="704" height="390" alt="image" src="https://github.com/user-attachments/assets/668ab1aa-6195-47ce-819c-6934634d4db6" />

2. **Calculating Accuracy**
   * **Loss Function**: Cross-entropy loss (same as pretraining) is used to optimize the model.
   * **Key Adjustment**: Loss is computed only on the last token’s output (```model(input_batch)[:, -1, :]```), as it contains the aggregated sequence information.

## Fine-Tuning Execution & Results

Execution and evaluation of the fine-tuning process, is the final step in transforming the pretrained model into a specialized spam classifier. The heart of this step is a function like ```train_classifier_simple```. This function orchestrates the iterative process of learning from the labeled dataset. Its main steps are:
1. **Iteration**: It loops over the entire training dataset for a set number of epochs (e.g. 5). Each epoch represents one full pass through the training data.
2. **Batch Processing**: For each epoch, it processes data in batches (e.g. groups of 8 text messages). This is efficient for both memory and computation.
3. **Core Training Steps (per batch)**:
   * **Forward Pass**: The batch of text is fed through the model.
   * **Loss Calculation**: The loss (cross-entropy) is calculated by comparing the model's prediction (from the last token) to the true label.
   * **Backward Pass**: The gradients of the loss with respect to the model's trainable parameters are computed.
   * **Optimization**: The optimizer (**AdamW**) uses these gradients to update the model's weights, slightly improving its ability to classify spam.
4. **Tracking**: It keeps track of examples seen and the global training step. This reflects the shift from an unsupervised (predict next token) to a supervised (predict a label) learning task.

The function includes crucial mechanisms to monitor the training progress and prevent overfitting:

* **Periodic Validation**: During training (```eval_freq=50```), it periodically checks the loss on the validation set. This allows you to see how the model performs on data it hasn't trained on.
* **Epoch-End Evaluation**: After each full epoch, it calculates the accuracy on both training and validation sets. The printed output (e.g., Training accuracy: 70.00% | Validation accuracy: 72.50%) provides a clear, human-readable progress report.

<img width="471" height="532" alt="image" src="https://github.com/user-attachments/assets/fe2c3e15-2a3c-47e5-a8eb-5c1b910807ad" />




