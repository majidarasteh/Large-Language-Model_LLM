# Fine Tuning for Classification
In this chapter, we transition from building and pretraining a general-purpose language model to specializing it for a concrete real-world task: **text classification**. We focus on fine-tuning a pretrained LLM to distinguish between “spam” and “not spam” messages—a classic binary classification problem. You will learn the key differences between instruction fine-tuning, which enables models to perform diverse tasks based on natural language prompts, and classification fine-tuning, which tailors a model to predict specific, predefined labels. Through hands-on examples, we cover the entire process: preparing a labeled dataset, modifying the model’s architecture, implementing the training loop, evaluating performance, and deploying the fine-tuned classifier. By the end of this chapter, you will have the skills to adapt LLMs for specialized classification tasks across various domains.

## Two Primary Fine-Tuning Paradigms
1. **Instruction Fine-Tuning**: Creates a generalist model that follows instructions. For example, the model can be asked "Is this spam?" or "Translate this to German." This flexibility is the key takeaway.
2. **Classification Fine-Tuning**: Creates a specialist model that maps inputs to a fixed set of predefined labels (spam/ham). Its strength is its narrow focus and simplicity for the task at hand.

The concept of a **specialist vs. a generalist** is a powerful and intuitive way to frame the difference. It helps readers understand the trade-off:
1. **Specialist (Classification-Tuned)**: Easier and cheaper to build, highly accurate at its specific job, but inflexible. It's a master of one trade.
2. **Generalist (Instruction-Tuned)**: More complex and expensive to build, versatile across many tasks, but might not be the absolute best at any single one compared to a specialist. It's a jack-of-all-trades.

The following figure illustrates two distinct scenarios for instruction fine-tuning and classification fine-tuning. This contrast demonstrates the core strength of instruction fine-tuning: it produces a generalist model capable of understanding and executing a wide range of tasks expressed through natural language prompts, rather than being limited to a single predefined function like a spam classifier.

<img width="911" height="405" alt="image" src="https://github.com/user-attachments/assets/11b0770f-6420-426e-a41d-30b816a8b536" />

### How this Sets Up the Rest of the Chapter

The chapter will now logically progress through the steps of creating a specialist model:
1. **Data Preparation**: How to get and prepare a dataset of spam/ham messages.
2. **Model Modification**: How to surgically alter the general-purpose GPT architecture (changing the output layer) to make it a specialist classifier.
3. **The Fine-Tuning Process**: How to train this modified model on the new data.
4. **Evaluation**: How to measure the performance of our new specialist.
5. **Usage**: How to use the finished model to classify new messages.

