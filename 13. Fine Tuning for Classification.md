# Fine Tuning for Classification
In this chapter, we transition from building and pretraining a general-purpose language model to specializing it for a concrete real-world task: **text classification**. We focus on fine-tuning a pretrained LLM to distinguish between “spam” and “not spam” messages—a classic binary classification problem. You will learn the key differences between instruction fine-tuning, which enables models to perform diverse tasks based on natural language prompts, and classification fine-tuning, which tailors a model to predict specific, predefined labels. Through hands-on examples, we cover the entire process: preparing a labeled dataset, modifying the model’s architecture, implementing the training loop, evaluating performance, and deploying the fine-tuned classifier. By the end of this chapter, you will have the skills to adapt LLMs for specialized classification tasks across various domains.

## Two Primary Fine-Tuning Paradigms
1. **Instruction Fine-Tuning**: Creates a generalist model that follows instructions. For example, the model can be asked "Is this spam?" or "Translate this to German." This flexibility is the key takeaway.
2. **Classification Fine-Tuning**: Creates a specialist model that maps inputs to a fixed set of predefined labels (spam/ham). Its strength is its narrow focus and simplicity for the task at hand.

The concept of a **specialist vs. a generalist** is a powerful and intuitive way to frame the difference. It helps readers understand the trade-off:
1. **Specialist (Classification-Tuned)**: Easier and cheaper to build, highly accurate at its specific job, but inflexible. It's a master of one trade.
2. **Generalist (Instruction-Tuned)**: More complex and expensive to build, versatile across many tasks, but might not be the absolute best at any single one compared to a specialist. It's a jack-of-all-trades.

The following figure illustrates two distinct scenarios for instruction fine-tuning and classification fine-tuning. This contrast demonstrates the core strength of instruction fine-tuning: it produces a generalist model capable of understanding and executing a wide range of tasks expressed through natural language prompts, rather than being limited to a single predefined function like a spam classifier.

<img width="911" height="405" alt="image" src="https://github.com/user-attachments/assets/11b0770f-6420-426e-a41d-30b816a8b536" />

### How this Sets Up the Rest of the Chapter

The chapter will now logically progress through the steps of creating a specialist model:
1. **Data Preparation**: How to get and prepare a dataset of spam/ham messages.
2. **Model Modification**: How to surgically alter the general-purpose GPT architecture (changing the output layer) to make it a specialist classifier.
3. **The Fine-Tuning Process**: How to train this modified model on the new data.
4. **Evaluation**: How to measure the performance of our new specialist.
5. **Usage**: How to use the finished model to classify new messages.

## Data Preparation

The first step in our classification task is to prepare the dataset:

* **Dataset**: We begin by downloading a real-world collection of SMS messages labeled as "spam" or "ham" (non-spam).
* **Imbalance Handling**: A common challenge in machine learning is dealing with imbalanced data, and this dataset is no exception, containing significantly more "ham" messages than "spam."
* **Balancing**: To ensure our model learns to identify both classes effectively and isn't biased toward the majority class, we rectify this imbalance by **undersampling** the "ham" instances to create a balanced dataset with an equal number of spam and non-spam examples.
* **Train/Validation/Test Split**: Finally, we split this refined dataset into standard **training (70%), validation (10%), and test (20%)** sets. This partitioning allows us to train the model, tune its parameters, and ultimately evaluate its performance on completely unseen data, providing a robust foundation for the fine-tuning process.

### Creating data loaders
To efficiently feed the text data into the model during training, we create PyTorch DataLoaders. Since messages have different lengths, we must standardize them for batching. Here the core problem is the variable-length input:
1. **Past Approach**: Previously, text was split into uniform chunks (e.g., using a sliding window), so every input was the same size, making them easy to batch together.
2. **New Challenge**: Individual text messages (like SMS) are all different lengths. A neural network requires inputs within a batch to be the same dimensions.

To solve the variable-length problem, the text must be standardized. The two common methods are:

1. **Truncation**: Chopping off the end of longer messages to make all messages as short as the shortest one.
   * **Pro**: Fast and uses less memory.
   * **Con**: You lose information! If the important part of a message is at the end, it gets thrown away, which can hurt the model's performance.
     
2. **Padding**: Adding a special, meaningless padding token to the end of shorter messages to make them all as long as the longest message.
   * **Pro**: Preserves all the original information in every message.
   * **Con**: Uses more memory and computation because the model has to process all those extra padding tokens.

We chose padding to avoid losing any important information from the messages. The technical implementation is as follow:
1. The model doesn't understand words; it understands numbers (token IDs).
2. Therefore, we don't pad with the word ```<|endoftext|>```. Instead, we pad with its corresponding **token ID: 50256**.
3. This is done after the text has been converted into a list of numbers (tokenized). So, a short message like ```["Hi", "there"]``` (IDs ```[123, 456]```) might become ```[123, 456, 50256, 50256, 50256]``` to match the length of a longer message.

This process is about making all text messages the same length by adding a special **"dummy" number (50256)** to the end of shorter messages. This allows the computer to efficiently process many messages at once in a batch without losing any of the original text content. For example the following figure shows padding:

<img width="911" height="459" alt="image" src="https://github.com/user-attachments/assets/370d2a6c-d16b-46ff-aca5-2473e92f8fd1" />


In the text classification, there is a fundamental shift in the model's objective and its original design. There is a shift from next token prediction to classification.  

* **Original Purpose (Pretraining):** The model was trained to predict the next token in a sequence.
* **New Purpose (Fine-tuning for Classification):** he model is now trained to predict a single, overall class label for an entire input sequence.

In the text classification, as an example, the structure of the data that will be fed into the model during training could be as follow:

* **input_batch**: A tensor with dimensions [8, 120].
  - **8** is the batch size (8 independent text messages processed together).
  - **120** is the sequence length. Each message has been padded/truncated to be exactly 120 tokens long. This is the uniform length required for batching.
    
* **target_batch**: A tensor with dimensions [8].
  - This is not a sequence. It's a simple list of 8 numbers.
  - Each number is the correct class label (0 or 1) for the corresponding message in the input_batch.

<img width="640" height="585" alt="image" src="https://github.com/user-attachments/assets/1d157efc-d277-4a6e-961b-ede0e9f1849d" />

