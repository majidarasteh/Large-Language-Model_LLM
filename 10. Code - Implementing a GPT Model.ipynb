{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d94bb15-d83b-4313-b2a8-c61a717e1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "    1. Layer Normalization Implementation\n",
    "    * A crucial component for stabilizing deep neural networks like Transformers.\n",
    "    * Purpose: Normalizes input tensors to have zero mean and unit variance along the last dimension (feature dimension). \n",
    "    * Stabilizes training and accelerates convergence.\n",
    "    * Usage: Commonly applied before/after attention and feed-forward layers in Transformers.\n",
    "\n",
    "    # Inherits from nn.Module, making it a custom PyTorch layer.\n",
    "    # emb_dim: The dimension of the input embeddings (e.g., 768 in GPT-2).\n",
    "    # eps: A small constant to prevent division by zero in variance calculations.\n",
    "    # scale and shift: Learnable parameters (γ and β in the formula).\n",
    "       * scale (γ) starts as a vector of ones (initially preserves the input scale).\n",
    "       * shift (β) starts as a vector of zeros (initially preserves the input center).\n",
    "       \n",
    "    # x.mean(dim=-1, keepdim=True): Calculates the mean along the last dimension (e.g., embedding dimension).\n",
    "       * keepdim=True ensures the output has the same number of dimensions (e.g., shape [batch, seq_len, 1]).\n",
    "    # x.var(dim=-1, keepdim=True, unbiased=False): Computes variance along the last dimension. \n",
    "       * unbiased=False uses a biased estimator (divides by n instead of n-1), which is standard in deep learning.\n",
    "\n",
    "    # norm_x: Standardizes the input to have mean 0 and variance 1. The eps prevents numerical instability.\n",
    "\n",
    "    # self.scale * norm_x + self.shift: Applies learnable parameters to transform the normalized data. \n",
    "      * This allows the model to adaptively adjust the output distribution.\n",
    "\n",
    "    # Example:\n",
    "      * Input x: Tensor of shape [batch_size, seq_len, emb_dim].\n",
    "      * Output: Tensor of the same shape, normalized and transformed.\n",
    "\"\"\"\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2194ee3c-caff-4dfe-8545-0c33401fab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2. GELU Activation Function\n",
    "    * Unlike ReLU (which has a sharp corner at 0), GELU is smooth and differentiable everywhere, leading to better gradient flow.\n",
    "\n",
    "    # No learnable parameters are needed, so the constructor simply calls the parent class initializer.\n",
    "    # GELU(x) ≈ 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x³))): This implements the approximate GELU formula\n",
    "\n",
    "    * In PyTorch:\n",
    "      the def forward(self, x): method is the core function of any custom neural network module (class that inherits from nn.Module). \n",
    "      It defines the actual computation that happens when you pass input data through the model.\n",
    "\"\"\"\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626a740e-ae69-424b-955e-bdc920b7727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    3. FeedForward Network\n",
    "    \n",
    "    A. Expansion Layer:\n",
    "       * nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim'])\n",
    "       * Increases the dimensionality of each token's representation by a factor of 4.\n",
    "       * Example: If emb_dim = 768, this layer expands it to 3072 dimensions.\n",
    "       * Why? Creates a higher-dimensional space where complex patterns can be learned.\n",
    "\n",
    "    B. Activation Function:\n",
    "       * GELU(): Applies the Gaussian Error Linear Unit activation.\n",
    "       * Introduces non-linearity, allowing the network to learn complex functions.\n",
    "       * Why GELU? Its smooth nature provides better gradients than ReLU for deep networks.\n",
    "\n",
    "    C. Contraction Layer\n",
    "       * nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
    "       * Projects the expanded representation back down to the original dimensionality.\n",
    "       * Example: From 3072 dimensions back to 768.\n",
    "       * Why? Maintains consistent input/output dimensions, allowing the block to be stacked.    \n",
    "\n",
    "    D. Forward Pass\n",
    "       * Simply passes the input through the sequential layers.\n",
    "       * Input shape: [batch_size, seq_len, emb_dim]\n",
    "       * Output shape: [batch_size, seq_len, emb_dim] (same as input)\n",
    "\n",
    "    E. Example \n",
    "       * Expansion: 768 → 3072 parameters\n",
    "       * Contraction: 3072 → 768 parameters\n",
    "       * Total parameters in FFN: ~(768×3072 + 3072×768) ≈ 4.7 million parameters per block\n",
    "\"\"\"\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f8fe37-63e7-4531-9bfa-979e06839483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    4. Multi-Head Attention\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Implementing multi-head attention with weight splits\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensures the output dimension can be split evenly among heads\n",
    "        # Example: If d_out=8 and num_heads=4, each head gets 2 dimensions\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads    \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)# Single large projection matrices (more efficient than separate ones per head)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   # Optional layer to mix information from different heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)         # [b, num_tokens, d_out]\n",
    "        queries = self.W_query(x)    # [b, num_tokens, d_out]\n",
    "        values = self.W_value(x)     # [b, num_tokens, d_out]\n",
    "\n",
    "        \"\"\"\n",
    "            Reshape for Multiple Heads\n",
    "            Reshapes [b, T, d_out] → [b, T, h, d_h] where d_out = h * d_h\n",
    "            Example: [2, 6, 8] → [2, 6, 4, 2] (4 heads, 2 dims each)\n",
    "        \"\"\"\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim                    \n",
    "        )                                                                   \n",
    "\n",
    "        \"\"\"\n",
    "            Transpose for Batch Computation\n",
    "            Rearranges to [batch, heads, tokens, dims_per_head]\n",
    "            Allows parallel computation across heads\n",
    "        \"\"\"\n",
    "        keys = keys.transpose(1, 2)          # [b, h, T, d_h]\n",
    "        queries = queries.transpose(1, 2)    # [b, h, T, d_h]\n",
    "        values = values.transpose(1, 2)      # [b, h, T, d_h]\n",
    "\n",
    "        \"\"\"\n",
    "            Compute Attention Scores\n",
    "            Batched matrix multiplication across all heads\n",
    "            Computes all attention scores in parallel\n",
    "        \"\"\"\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "\n",
    "        \"\"\"\n",
    "            Apply Causal Mask\n",
    "            Uses pre-computed triangular mask\n",
    "            Blocks future tokens for autoregressive generation\n",
    "        \"\"\"\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        \n",
    "        # Softmax: Standard scaled softmax attention\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        \"\"\"\n",
    "            Apply Attention to Values\n",
    "            Weighted sum of values\n",
    "            ranspose back to [batch, tokens, heads, dims]\n",
    "        \"\"\"\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)   # [b, T, h, d_h]\n",
    "\n",
    "        \"\"\"\n",
    "            Combine Heads\n",
    "            Flatten heads: [b, T, h, d_h] → [b, T, h*d_h] = [b, T, d_out]\n",
    "            Example: [2, 6, 4, 2] → [2, 6, 8]\n",
    "        \"\"\"\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "            Output Projection\n",
    "            Optional linear transformation\n",
    "            Helps mix information across heads\n",
    "        \"\"\"\n",
    "        context_vec = self.out_proj(context_vec)    #11\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d776b049-322c-4a93-8540-29f82f207b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    5. Transformer Block\n",
    "      * Transformer Block is the fundamental building block of models like GPT.\n",
    "      * It combines self-attention and feed-forward layers with normalization and residual connections. \n",
    "\n",
    "      A. self.att: Multi-head self-attention mechanism.\n",
    "        * d_in=d_out=cfg['emb_dim']: Input/output dimensions match (e.g., 768).\n",
    "        * context_length: Maximum sequence length (for causal masking).\n",
    "        * num_heads: Number of attention heads (e.g., 12).\n",
    "        * dropout: Dropout rate for attention weights.\n",
    "        * qkv_bias: Whether to use bias in query/key/value projections.\n",
    "        \n",
    "      B. self.ff: Feed-forward network (expands to 4× dims, then contracts).\n",
    "      C. self.norm1, self.norm2: Layer normalization applied before attention and FFN (Pre-LN architecture).\n",
    "      D. self.drop_shortcut: Dropout applied to the output of sub-layers before adding to the shortcut.\n",
    "      \n",
    "\"\"\"\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg['emb_dim'],\n",
    "            d_out=cfg['emb_dim'],\n",
    "            context_length=cfg['context_length'],\n",
    "            num_heads=cfg['n_heads'], \n",
    "            dropout=cfg['drop_rate'],\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with shortcut connection\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        # Feed-forward with shortcut connection\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f13a6f2-8b9a-43d9-8d54-c4acf1ab9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    6. GPT Model Architecture\n",
    "    * This code defines the complete GPT model architecture, which is the core of generative language models like GPT-2/GPT-3.\n",
    "\n",
    "    A. Initialization (__init__)\n",
    "      * tok_emb: Token embedding layer. Converts token IDs (integers) to dense vectors of size emb_dim.\n",
    "      * pos_emb: Position embedding layer.\n",
    "        Adds information about token positions (learnable embeddings for each position up to context_length).\n",
    "      * drop_emb: Dropout applied to the combined embeddings for regularization.\n",
    "      * trf_blocks: A stack of identical TransformerBlock layers (e.g., 12 layers for GPT-2 Small). \n",
    "        This is the core processing unit.\n",
    "      * final_norm: Final layer normalization for stability.\n",
    "      * out_head: Linear layer that projects final hidden states to vocabulary-sized logits (scores for each token).\n",
    "\n",
    "    B Forward Pass (forward)\n",
    "      Step 1: Token Embeddings: \n",
    "        * Converts input token IDs (e.g., [batch_size, seq_len]) to dense embeddings.\n",
    "        \n",
    "      Step 2: Position Embeddings: \n",
    "        * Generates position embeddings for each position in the sequence (0 to seq_len-1).\n",
    "        * These are added to token embeddings to give the model information about word order.\n",
    "        \n",
    "      Step 3: Combine and Regularize:\n",
    "        * The combination allows each token representation to encode both semantic and positional information.\n",
    "        \n",
    "      Step 4: Transformer Blocks:\n",
    "        * The input passes through multiple TransformerBlock layers (e.g., 12).\n",
    "        * Each block applies:\n",
    "          - Multi-head self-attention (contextualization)\n",
    "          - Feed-forward network (feature transformation)\n",
    "          - Residual connections and normalization\n",
    "          \n",
    "      Step 5: Final Output:\n",
    "         * Final normalization: Stabilizes the outputs.\n",
    "         * Output projection: Converts final hidden states to logits (scores) for each token in the vocabulary.\n",
    "\n",
    "      * Input/Output\n",
    "        - Input: Integer tensor of token IDs, shape [batch_size, seq_len].\n",
    "        - Output: Logits tensor, shape [batch_size, seq_len, vocab_size].\n",
    "             * For each position in the sequence, returns scores for all possible next tokens.\n",
    "\n",
    "      * Stacked Transformers\n",
    "         - Multiple blocks process the sequence iteratively:\n",
    "           - Early layers: Capture local patterns and syntax.\n",
    "           - Middle layers: Build semantic understanding.\n",
    "           - Later layers: Develop complex reasoning.         \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'], cfg['vocab_size'], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        \n",
    "        # Position embeddings\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.trf_blocks(x)\n",
    "        \n",
    "        # Final normalization and output\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ff3962-d7e1-487c-9d52-93e639d0259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    7. Model Configuration\n",
    "    \n",
    "    * This code defines a configuration dictionary for the GPT-2 Small model (124 million parameters).\n",
    "    * Each parameter controls a specific aspect of the model's architecture and behavior:\n",
    "    \n",
    "    * 'vocab_size': 50257: \n",
    "      - Meaning: The number of unique tokens in the model's vocabulary.\n",
    "      - Purpose: Determines the size of the token embedding layer and the output projection.\n",
    "      - Note: 50,257 is the vocabulary size used by GPT-2's Byte Pair Encoding (BPE) tokenizer.\n",
    "\n",
    "    * 'context_length': 1024\n",
    "      - Meaning: The maximum sequence length (in tokens) the model can process.\n",
    "      - Purpose: Defines the maximum input length for positional embeddings and the causal attention mask.\n",
    "      - Implication: Inputs longer than 1024 tokens must be truncated or chunked.\n",
    "\n",
    "    * 'emb_dim': 768\n",
    "      - Meaning: The dimensionality of token and position embeddings.\n",
    "      - Purpose: Controls the size of all hidden representations throughout the model.\n",
    "      - Calculation: Affects the size of all linear layers and the attention mechanism.\n",
    "\n",
    "    * 'n_heads': 12\n",
    "      - Meaning: The number of parallel attention heads.\n",
    "      - Purpose: Allows the model to focus on different types of linguistic patterns simultaneously.\n",
    "      - Constraint: Must evenly divide emb_dim (768 ÷ 12 = 64 dimensions per head).\n",
    "\n",
    "    * 'n_layers': 12\n",
    "      - Meaning: The number of transformer blocks stacked sequentially.\n",
    "      - Purpose: Determines the depth of the model. Each layer refines the representations further.\n",
    "      - Note: More layers generally increase capacity but also computational cost.\n",
    "\n",
    "    * 'drop_rate': 0.1\n",
    "      - Meaning: The dropout probability (10% of activations are randomly zeroed during training).\n",
    "      - Purpose: Regularization to prevent overfitting.\n",
    "      - Applied: In attention weights, feed-forward networks, and embeddings.\n",
    "\n",
    "    * 'qkv_bias': False\n",
    "      - Meaning: Whether to include bias terms in the Query, Key, and Value linear projections.\n",
    "      - Purpose: Small optimization that reduces parameters slightly (768 × 3 biases omitted).\n",
    "      - Note: Following GPT-2's original design choice.\n",
    "\n",
    "    * Architectural Implications:\n",
    "      - Token Embeddings: 50257 × 768 parameters\n",
    "      - Position Embeddings: 1024 × 768 parameters\n",
    "      - Feed-Forward Expansion: 768 × 4 = 3072 hidden dimensions in FFN\n",
    "      - Attention Head Dimension: 768 ÷ 12 = 64 dimensions per head\n",
    "      - Total Parameters: ~124 million\n",
    "\"\"\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50257,     # Vocabulary size\n",
    "    'context_length': 1024,  # Context length\n",
    "    'emb_dim': 768,          # Embedding dimension\n",
    "    'n_heads': 12,           # Number of attention heads\n",
    "    'n_layers': 12,          # Number of layers\n",
    "    'drop_rate': 0.1,        # Dropout rate\n",
    "    'qkv_bias': False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b664592-8640-463c-b86e-b6af87ca3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    8. Text Generation Function\n",
    "    * This code implements a simple greedy autoregressive text generation function for GPT-like models. \n",
    "      It generates text one token at a time by always choosing the most likely next token. \n",
    "\n",
    "    * model: The trained GPT model.\n",
    "    * idx: Input tensor of token indices (shape: [batch_size, num_tokens]).\n",
    "    * max_new_tokens: Number of new tokens to generate.\n",
    "    * context_size: Maximum context length the model can handle (e.g., 1024).\n",
    "\n",
    "    A. Loop for Token Generation\n",
    "     *  Loop for Token Generation:\n",
    "       - for _ in range(max_new_tokens): \n",
    "       - Generates one token per iteration until max_new_tokens are created.\n",
    "       \n",
    "     * Context Cropping: \n",
    "       - Truncates the input to the last context_size tokens.\n",
    "       - Why? GPT models have a fixed context window. If the sequence exceeds this, we only use the most recent tokens.\n",
    "       \n",
    "     * Model Prediction:Feeds the cropped context through the model to get predictions (logits).\n",
    "       - logits shape: [batch_size, context_size, vocab_size]\n",
    "       \n",
    "     * Focus on Last Token\n",
    "      - logits = logits[:, -1, :]\n",
    "      - Extracts the logits for only the last token position.\n",
    "      - Why? We only care about the next-token prediction.\n",
    "      - New shape: [batch_size, vocab_size]\n",
    "      \n",
    "     * Convert to Probabilities\n",
    "      - probas = torch.softmax(logits, dim=-1)\n",
    "      - Applies softmax to convert logits to probabilities.\n",
    "      - probas shape: [batch_size, vocab_size] (sums to 1 along vocabulary dimension).\n",
    "\n",
    "     * Greedy Sampling\n",
    "      - idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "      - Selects the token with the highest probability.\n",
    "      - keepdim=True preserves dimensions: [batch_size, 1] instead of [batch_size].\n",
    "\n",
    "     * Append New Token\n",
    "      - idx = torch.cat((idx, idx_next), dim=1)\n",
    "      - Appends the new token to the existing sequence.\n",
    "      - The sequence grows by one token each iteration.\n",
    "\n",
    "     * Return Result\n",
    "      - return idx\n",
    "      - Returns the extended sequence containing both original input and generated tokens.\n",
    "\"\"\"\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "\n",
    "    # Loop for Token Generation\n",
    "    # Generates one token per iteration until max_new_tokens are created.\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop context if needed\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus on last time step\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample next token (greedy)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Append to sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a3f5e9d-c4eb-4867-8ee6-e0a68accbd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 163,009,536\n",
      "Model size: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    9. Parameter Counting and Memory Calculation\n",
    "    * Calculate the number of parameters in a PyTorch model and estimate its memory usage. \n",
    "\"\"\"\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def calculate_memory_usage(model, dtype_size=4):\n",
    "    total_params = count_parameters(model)\n",
    "    total_size_bytes = total_params * dtype_size\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    return total_params, total_size_mb\n",
    "\n",
    "# Example usage\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "total_params, total_size_mb = calculate_memory_usage(model)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48da1feb-582f-4f02-9b64-9dc8533c7330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "Total parameters: 163,009,536\n",
      "Model size: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    10. Testing the Implementation\n",
    "\"\"\"\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Initialize model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "batch = []\n",
    "txt1 = 'Every effort moves you'\n",
    "txt2 = 'Every day holds a'\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "\n",
    "# Count parameters\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate memory usage\n",
    "_, total_size_mb = calculate_memory_usage(model)\n",
    "print(f\"Model size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "237168d0-3988-4ed1-98ab-446a4c32f654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = 'Hello, I am'\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print('encoded:', encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)    #1\n",
    "print('encoded_tensor.shape:', encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61804a3f-4fcf-4d1e-9155-3e07d14be1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 15309, 37606, 20783, 24901, 36086, 48535]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()                  #1\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "print('Output:', out)\n",
    "print('Output length:', len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91957798-c223-4fca-b31b-2069823778c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I amAlexittowalker legends Duo Frankie\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "As we can see, the model generated gibberish, which is not at all like the coherent text Hello, I am a model ready to help.\n",
    "What happened? The reason the model is unable to produce coherent text is that we haven’t trained it yet. \n",
    "So far, we have only implemented the GPT architecture and initialized a GPT model instance with initial random weights.\n",
    "Model training is a large topic in itself, and we will tackle it in the next chapter.\n",
    "\"\"\"\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
